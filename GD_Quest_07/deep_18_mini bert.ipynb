{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32a3924d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import collections\n",
    "import json\n",
    "import shutil\n",
    "import zipfile\n",
    "import copy\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import sentencepiece as spm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "random_seed = 1234\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)\n",
    "\n",
    "# tf version 및 gpu 확인\n",
    "print(tf.__version__)\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "print(tf.test.gpu_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e73c03a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "완료=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=/aiffel/aiffel/bert_pretrain/data/kowiki.txt --model_prefix=ko_8000 --vocab_size=8007 --model_type=bpe --max_sentence_length=999999 --pad_id=0 --pad_piece=[PAD] --unk_id=1 --unk_piece=[UNK] --bos_id=2 --bos_piece=[BOS] --eos_id=3 --eos_piece=[EOS] --user_defined_symbols=[SEP],[CLS],[MASK]\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: /aiffel/aiffel/bert_pretrain/data/kowiki.txt\n",
      "  input_format: \n",
      "  model_prefix: ko_8000\n",
      "  model_type: BPE\n",
      "  vocab_size: 8007\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 999999\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  user_defined_symbols: [SEP]\n",
      "  user_defined_symbols: [CLS]\n",
      "  user_defined_symbols: [MASK]\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 1\n",
      "  bos_id: 2\n",
      "  eos_id: 3\n",
      "  pad_id: 0\n",
      "  unk_piece: [UNK]\n",
      "  bos_piece: [BOS]\n",
      "  eos_piece: [EOS]\n",
      "  pad_piece: [PAD]\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: /aiffel/aiffel/bert_pretrain/data/kowiki.txt\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 1000000 lines\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 2000000 lines\n",
      "trainer_interface.cc(117) LOG(WARNING) Too many sentences are loaded! (2451287), which may slow down training.\n",
      "trainer_interface.cc(119) LOG(WARNING) Consider using --input_sentence_size=<size> and --shuffle_input_sentence=true.\n",
      "trainer_interface.cc(122) LOG(WARNING) They allow to randomly sample <size> sentences from the entire corpus.\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 2451287 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [PAD]\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [UNK]\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [BOS]\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [EOS]\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [SEP]\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [CLS]\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [MASK]\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=287452241\n",
      "trainer_interface.cc(477) LOG(INFO) Done: 99.95% characters are covered.\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=4411\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=0.9995\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 2450254 sentences.\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 2450254\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 7050692\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=1781571 min_freq=424\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=576838 size=20 all=581927 active=38577 piece=▁아\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=390836 size=40 all=591445 active=48095 piece=▁유\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=297873 size=60 all=601378 active=58028 piece=에는\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=244712 size=80 all=609974 active=66624 piece=▁성\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=194372 size=100 all=616449 active=73099 piece=까지\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=193674 min_freq=462\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=176838 size=120 all=625299 active=38770 piece=▁우\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=154294 size=140 all=632274 active=45745 piece=▁파\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=140625 size=160 all=639734 active=53205 piece=00\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=125983 size=180 all=645481 active=58952 piece=▁요\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=114855 size=200 all=649839 active=63310 piece=리아\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=114086 min_freq=457\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=106760 size=220 all=657338 active=39316 piece=▁같은\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=100770 size=240 all=662564 active=44542 piece=▁왕\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=96037 size=260 all=670536 active=52514 piece=▁목\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=88392 size=280 all=675441 active=57419 piece=▁f\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=81678 size=300 all=681701 active=63679 piece=▁선수\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=80870 min_freq=446\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=77144 size=320 all=686930 active=39163 piece=▁때문에\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=73218 size=340 all=691000 active=43233 piece=▁조선\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=68829 size=360 all=695717 active=47950 piece=▁천\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=64009 size=380 all=700839 active=53072 piece=▁196\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=60953 size=400 all=706675 active=58908 piece=▁돌\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=60762 min_freq=435\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=58542 size=420 all=711350 active=39712 piece=▁다시\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=55779 size=440 all=715377 active=43739 piece=▁K\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=52528 size=460 all=721839 active=50201 piece=▁모두\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=49784 size=480 all=727356 active=55718 piece=▁히\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=47637 size=500 all=733038 active=61400 piece=▁전쟁\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=47558 min_freq=423\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=45919 size=520 all=738287 active=41703 piece=▁있어\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=44025 size=540 all=743886 active=47302 piece=▁중심\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=42378 size=560 all=748357 active=51773 piece=▁N\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=40922 size=580 all=752114 active=55530 piece=▁H\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=39922 size=600 all=755142 active=58558 piece=le\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=39768 min_freq=414\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=38924 size=620 all=761204 active=43650 piece=▁검\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=37771 size=640 all=767376 active=49822 piece=란드\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=36292 size=660 all=772837 active=55283 piece=정을\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=35134 size=680 all=778715 active=61161 piece=▁설립\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=34016 size=700 all=783719 active=66165 piece=▁역사\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=34003 min_freq=400\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=33144 size=720 all=787243 active=42507 piece=▁만들어\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=32383 size=740 all=791538 active=46802 piece=▁시간\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=31645 size=760 all=795131 active=50395 piece=▁측\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=30941 size=780 all=798845 active=54109 piece=과의\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=30041 size=800 all=803050 active=58314 piece=도는\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=30023 min_freq=392\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=29448 size=820 all=808546 active=45099 piece=▁난\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=28836 size=840 all=813895 active=50448 piece=▁21\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=28270 size=860 all=818654 active=55207 piece=▁찾\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=27299 size=880 all=824625 active=61177 piece=되지\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=26862 size=900 all=828285 active=64837 piece=하자\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=26833 min_freq=381\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=26060 size=920 all=832595 active=45199 piece=부에\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=25504 size=940 all=838824 active=51428 piece=수의\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=24726 size=960 all=843322 active=55926 piece=▁남아\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=24083 size=980 all=848416 active=61020 piece=▁않는다\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=23647 size=1000 all=853601 active=66205 piece=인민\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=23602 min_freq=368\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=23176 size=1020 all=859529 active=48367 piece=▁마지\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=22849 size=1040 all=863129 active=51967 piece=▁시리즈\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=22293 size=1060 all=868678 active=57516 piece=제로\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=21851 size=1080 all=873075 active=61913 piece=시의\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=21377 size=1100 all=878277 active=67115 piece=해야\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=21369 min_freq=355\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=21043 size=1120 all=882003 active=47093 piece=▁녹\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=20625 size=1140 all=886266 active=51356 piece=▁인구는\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=20185 size=1160 all=889382 active=54472 piece=ac\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=19762 size=1180 all=894738 active=59828 piece=인은\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=19447 size=1200 all=899565 active=64655 piece=50\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=19432 min_freq=347\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=19118 size=1220 all=903215 active=48471 piece=광역\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=18777 size=1240 all=908007 active=53263 piece=수는\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=18396 size=1260 all=913268 active=58524 piece=인민공\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=18040 size=1280 all=917389 active=62645 piece=▁긴\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=17659 size=1300 all=922177 active=67433 piece=▁전통\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=17629 min_freq=335\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=17404 size=1320 all=926354 active=50086 piece=▁망\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=17093 size=1340 all=929494 active=53226 piece=단의\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=16931 size=1360 all=934269 active=58001 piece=▁인간\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=16719 size=1380 all=939872 active=63604 piece=▁바로\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=16586 size=1400 all=943447 active=67179 piece=▁탑\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=16584 min_freq=327\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=16331 size=1420 all=947545 active=51069 piece=▁태양\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=16086 size=1440 all=952041 active=55565 piece=▁경기에서\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=15873 size=1460 all=957568 active=61092 piece=▁동일\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=15577 size=1480 all=962681 active=66205 piece=드를\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=15352 size=1500 all=966310 active=69834 piece=▁뮤\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=15328 min_freq=318\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=15103 size=1520 all=968851 active=50741 piece=id\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=14971 size=1540 all=972545 active=54435 piece=▁옥\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=14723 size=1560 all=977306 active=59196 piece=비전\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=14495 size=1580 all=982182 active=64072 piece=▁대부분의\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=14292 size=1600 all=985970 active=67860 piece=▁갑\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=14280 min_freq=311\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=14118 size=1620 all=990010 active=53173 piece=회는\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=13982 size=1640 all=993514 active=56677 piece=▁사고\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=13729 size=1660 all=997224 active=60387 piece=cm\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=13567 size=1680 all=1001600 active=64762 piece=▁뛰어\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=13433 size=1700 all=1006574 active=69736 piece=▁더욱\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=13420 min_freq=303\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=13135 size=1720 all=1011076 active=54820 piece=지역\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=12940 size=1740 all=1015431 active=59175 piece=▁선언\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=12821 size=1760 all=1020339 active=64083 piece=▁어려\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=12640 size=1780 all=1023221 active=66965 piece=▁칭\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=12505 size=1800 all=1028177 active=71921 piece=▁옛\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=12503 min_freq=295\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=12409 size=1820 all=1033793 active=56931 piece=ce\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=12253 size=1840 all=1037183 active=60321 piece=▁그들의\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=12091 size=1860 all=1041900 active=65038 piece=▁단체\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=11934 size=1880 all=1045379 active=68517 piece=▁예술\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=11799 size=1900 all=1048007 active=71145 piece=라의\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=11799 min_freq=288\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=11693 size=1920 all=1050313 active=54358 piece=▁불구하고\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=11594 size=1940 all=1053703 active=57748 piece=▁분리\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=11496 size=1960 all=1057998 active=62043 piece=▁사이의\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=11381 size=1980 all=1063418 active=67463 piece=단이\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=11289 size=2000 all=1067885 active=71930 piece=im\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=11277 min_freq=281\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=11133 size=2020 all=1071888 active=57167 piece=▁등과\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=10994 size=2040 all=1077153 active=62432 piece=법을\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=10895 size=2060 all=1081334 active=66613 piece=▁괴\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=10776 size=2080 all=1084885 active=70164 piece=러스\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=10692 size=2100 all=1089330 active=74609 piece=▁깨\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=10688 min_freq=274\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=10597 size=2120 all=1091576 active=56585 piece=릭터\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=10496 size=2140 all=1095180 active=60189 piece=▁확장\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=10396 size=2160 all=1100510 active=65519 piece=었던\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=10271 size=2180 all=1104334 active=69343 piece=▁남부\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=10143 size=2200 all=1108628 active=73637 piece=▁스포츠\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=10136 min_freq=268\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=10057 size=2220 all=1111848 active=58430 piece=▁이외\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=9957 size=2240 all=1115235 active=61817 piece=▁서식\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=9848 size=2260 all=1120216 active=66798 piece=▁작용\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=9732 size=2280 all=1124443 active=71025 piece=▁이야기\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=9655 size=2300 all=1129106 active=75688 piece=▁성립\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=9653 min_freq=261\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=9570 size=2320 all=1132064 active=59284 piece=▁떨어진\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=9473 size=2340 all=1135883 active=63103 piece=시를\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=9352 size=2360 all=1139681 active=66901 piece=▁히로\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=9289 size=2380 all=1142636 active=69856 piece=▁부정\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=9219 size=2400 all=1147544 active=74764 piece=▁2020\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=9217 min_freq=256\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=9137 size=2420 all=1150230 active=60032 piece=▁상징\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=9028 size=2440 all=1153890 active=63692 piece=▁1950\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=8965 size=2460 all=1157151 active=66953 piece=▁표시\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=8889 size=2480 all=1162268 active=72070 piece=▁사용된다\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=8828 size=2500 all=1165804 active=75606 piece=▁아일랜드\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=8826 min_freq=251\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=8738 size=2520 all=1169223 active=61615 piece=▁동생\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=8694 size=2540 all=1173546 active=65938 piece=ie\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=8641 size=2560 all=1176802 active=69194 piece=▁경상북도\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=8570 size=2580 all=1180886 active=73278 piece=▁계승\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=8489 size=2600 all=1184747 active=77139 piece=▁1985\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=8487 min_freq=246\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=8434 size=2620 all=1188193 active=62666 piece=력의\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=8345 size=2640 all=1191415 active=65888 piece=인지\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=8270 size=2660 all=1196857 active=71330 piece=▁달성\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=8223 size=2680 all=1200791 active=75264 piece=▁충돌\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=8139 size=2700 all=1202837 active=77310 piece=ak\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=8133 min_freq=241\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=8061 size=2720 all=1205959 active=63019 piece=▁2,\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=8008 size=2740 all=1208089 active=65149 piece=)\"\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=7956 size=2760 all=1210710 active=67770 piece=▁둘러\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=7894 size=2780 all=1213790 active=70850 piece=워드\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=7817 size=2800 all=1216934 active=73994 piece=▁즐"
     ]
    }
   ],
   "source": [
    "# 실행하길 원한다면 \"\"\" 를 지워주세요.\n",
    "import sentencepiece as spm\n",
    "import os\n",
    "corpus_file = os.getenv('HOME')+'/aiffel/bert_pretrain/data/kowiki.txt'\n",
    "prefix = 'ko_8000'\n",
    "vocab_size = 8000\n",
    "\n",
    "spm.SentencePieceTrainer.train(\n",
    "    f\"--input={corpus_file} --model_prefix={prefix} --vocab_size={vocab_size + 7}\" + \n",
    "    \" --model_type=bpe\" +\n",
    "    \" --max_sentence_length=999999\" + # 문장 최대 길이\n",
    "    \" --pad_id=0 --pad_piece=[PAD]\" + # pad (0)\n",
    "    \" --unk_id=1 --unk_piece=[UNK]\" + # unknown (1)\n",
    "    \" --bos_id=2 --bos_piece=[BOS]\" + # begin of sequence (2)\n",
    "    \" --eos_id=3 --eos_piece=[EOS]\" + # end of sequence (3)\n",
    "    \" --user_defined_symbols=[SEP],[CLS],[MASK]\") # 사용자 정의 토큰\n",
    "\n",
    "print(\"완료=3\")   # 완료메시지가 출력될 때까지 아무 출력내용이 없더라도 기다려 주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1a6d05e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = os.getenv('HOME')+'/aiffel/bert_pretrain/data'\n",
    "# model_dir = os.getenv('HOME')+'/aiffel/bert_pretrain/models'\n",
    "model_dir = os.getenv('HOME')+'/aiffel/prj'\n",
    "\n",
    "# vocab loading\n",
    "vocab = spm.SentencePieceProcessor()\n",
    "vocab.load(f\"{model_dir}/ko_8000.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f77bfe4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list = []\n",
    "for id in range(7, len(vocab)):\n",
    "    if not vocab.is_unknown(id):\n",
    "        vocab_list.append(vocab.id_to_piece(id))\n",
    "# print(vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7a224a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '번', '▁오', '십', '▁전', '▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에', '[SEP]', '▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러', '▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내', '▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# [CLS], tokens a, [SEP], tokens b, [SEP] 형태의 token 생성\n",
    "string_a = \"추적추적 비가 내리는 날이었어 그날은 왠지 손님이 많아 첫 번에 삼십 전 둘째번 오십 전 오랜만에 받아보는 십 전짜리 백통화 서푼에\"\n",
    "string_b = \"손바닥 위엔 기쁨의 눈물이 흘러 컬컬한 목에 모주 한잔을 적셔 몇 달 포 전부터 콜록거리는 아내 생각에 그토록 먹고 싶다던\"\n",
    "tokens_org = [\"[CLS]\"] + vocab.encode_as_pieces(string_a) + [\"[SEP]\"] + vocab.encode_as_pieces(string_b) + [\"[SEP]\"]\n",
    "print(tokens_org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "80368158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '번', '▁오', '십', '▁전', '▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에', '[SEP]', '▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러', '▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내', '▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '[SEP]']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tokens_org)\n",
    "\n",
    "# 전체 token의 15% mask\n",
    "mask_cnt = int((len(tokens_org) - 3) * 0.15)\n",
    "mask_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "69cbd162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4] ['▁추', '적', '추', '적']\n",
      "[5, 6] ['▁비', '가']\n",
      "[7, 8] ['▁내', '리는']\n",
      "[9, 10, 11] ['▁날', '이었', '어']\n",
      "[12, 13, 14] ['▁그', '날', '은']\n",
      "[15, 16, 17] ['▁', '왠', '지']\n",
      "[18, 19, 20] ['▁손', '님', '이']\n",
      "[21, 22] ['▁많', '아']\n",
      "[23] ['▁첫']\n",
      "[24, 25] ['▁번', '에']\n",
      "[26, 27] ['▁삼', '십']\n",
      "[28] ['▁전']\n",
      "[29, 30, 31] ['▁둘', '째', '번']\n",
      "[32, 33] ['▁오', '십']\n",
      "[34] ['▁전']\n",
      "[35, 36, 37] ['▁오', '랜', '만에']\n",
      "[38, 39, 40] ['▁받아', '보', '는']\n",
      "[41] ['▁십']\n",
      "[42, 43, 44] ['▁전', '짜', '리']\n",
      "[45, 46, 47] ['▁백', '통', '화']\n",
      "[48, 49, 50] ['▁서', '푼', '에']\n",
      "[52, 53, 54] ['▁손', '바', '닥']\n",
      "[55, 56] ['▁위', '엔']\n",
      "[57, 58, 59] ['▁기', '쁨', '의']\n",
      "[60, 61] ['▁눈', '물이']\n",
      "[62, 63] ['▁흘', '러']\n",
      "[64, 65, 66] ['▁컬', '컬', '한']\n",
      "[67, 68] ['▁목', '에']\n",
      "[69, 70] ['▁모', '주']\n",
      "[71, 72, 73] ['▁한', '잔', '을']\n",
      "[74, 75] ['▁적', '셔']\n",
      "[76] ['▁몇']\n",
      "[77] ['▁달']\n",
      "[78] ['▁포']\n",
      "[79, 80] ['▁전', '부터']\n",
      "[81, 82, 83, 84] ['▁콜', '록', '거', '리는']\n",
      "[85] ['▁아내']\n",
      "[86, 87] ['▁생각', '에']\n",
      "[88, 89, 90] ['▁그', '토', '록']\n",
      "[91, 92] ['▁먹', '고']\n",
      "[93, 94, 95] ['▁싶', '다', '던']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 띄어쓰기 단위로 mask하기 위해서 index 분할\n",
    "cand_idx = []  # word 단위의 index array\n",
    "for (i, token) in enumerate(tokens_org):\n",
    "    if token == \"[CLS]\" or token == \"[SEP]\":\n",
    "        continue\n",
    "    if 0 < len(cand_idx) and not token.startswith(u\"\\u2581\"):  # u\"\\u2581\"는 단어의 시작을 의미하는 값\n",
    "        cand_idx[-1].append(i)\n",
    "    else:\n",
    "        cand_idx.append([i])\n",
    "\n",
    "# 결과확인\n",
    "for cand in cand_idx:\n",
    "    print(cand, [tokens_org[i] for i in cand])\n",
    "    \n",
    "len(cand_idx)\n",
    "# tokens_org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b9f0a59a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[24, 25],\n",
       " [57, 58, 59],\n",
       " [32, 33],\n",
       " [64, 65, 66],\n",
       " [41],\n",
       " [79, 80],\n",
       " [52, 53, 54],\n",
       " [67, 68],\n",
       " [29, 30, 31],\n",
       " [91, 92],\n",
       " [23],\n",
       " [26, 27],\n",
       " [76],\n",
       " [42, 43, 44],\n",
       " [78],\n",
       " [60, 61],\n",
       " [38, 39, 40],\n",
       " [93, 94, 95],\n",
       " [9, 10, 11],\n",
       " [81, 82, 83, 84],\n",
       " [85],\n",
       " [12, 13, 14],\n",
       " [34],\n",
       " [71, 72, 73],\n",
       " [77],\n",
       " [45, 46, 47],\n",
       " [48, 49, 50],\n",
       " [28],\n",
       " [74, 75],\n",
       " [62, 63],\n",
       " [88, 89, 90],\n",
       " [5, 6],\n",
       " [35, 36, 37],\n",
       " [55, 56],\n",
       " [18, 19, 20],\n",
       " [86, 87],\n",
       " [7, 8],\n",
       " [15, 16, 17],\n",
       " [1, 2, 3, 4],\n",
       " [21, 22],\n",
       " [69, 70]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random mask를 위해서 index 순서를 섞음\n",
    "random.shuffle(cand_idx)\n",
    "cand_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9ab2432d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens_org\n",
      "['[CLS]', '▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '번', '▁오', '십', '▁전', '▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에', '[SEP]', '▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러', '▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내', '▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '[SEP]'] \n",
      "\n",
      "tokens\n",
      "['[CLS]', '▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '[MASK]', '[MASK]', '[MASK]', '▁삼', '십', '▁전', '▁둘', '째', '번', '[MASK]', '[MASK]', '▁전', '▁오', '랜', '만에', '▁받아', '보', '는', '프', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에', '[SEP]', '▁손', '바', '닥', '▁위', '엔', '[MASK]', '[MASK]', '[MASK]', '▁눈', '물이', '▁흘', '러', '[MASK]', '[MASK]', '[MASK]', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내', '▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# tokens가 mask되므로 재 실행을 위해서 넣어줌 (테스트용)\n",
    "tokens = copy.deepcopy(tokens_org)\n",
    "\n",
    "mask_lms = []  # mask 된 값\n",
    "for index_set in cand_idx:\n",
    "    if len(mask_lms) >= mask_cnt:  # 핸재 mask된 개수가 15%를 넘으면 중지\n",
    "          break\n",
    "    if len(mask_lms) + len(index_set) > mask_cnt:  # 이번에 mask할 개수를 포함해 15%를 넘으면 skip\n",
    "          continue\n",
    "    dice = random.random()  # 0과 1 사이의 확률 값\n",
    "\n",
    "    for index in index_set:\n",
    "        masked_token = None\n",
    "        if dice < 0.8:  # 80% replace with [MASK]\n",
    "            masked_token = \"[MASK]\"\n",
    "        elif dice < 0.9: # 10% keep original\n",
    "            masked_token = tokens[index]\n",
    "        else:  # 10% random word\n",
    "            masked_token = random.choice(vocab_list)\n",
    "        mask_lms.append({\"index\": index, \"label\": tokens[index]})\n",
    "        tokens[index] = masked_token\n",
    "\n",
    "print(\"tokens_org\")\n",
    "print(tokens_org, \"\\n\")\n",
    "print(\"tokens\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0634b203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask_idx   : [23, 24, 25, 32, 33, 41, 57, 58, 59, 64, 65, 66, 79, 80]\n",
      "mask_label : ['▁첫', '▁번', '에', '▁오', '십', '▁십', '▁기', '쁨', '의', '▁컬', '컬', '한', '▁전', '부터']\n"
     ]
    }
   ],
   "source": [
    "# 순서 정렬 및 mask_idx, mask_label 생성\n",
    "mask_lms = sorted(mask_lms, key=lambda x: x[\"index\"])\n",
    "mask_idx = [p[\"index\"] for p in mask_lms]\n",
    "mask_label = [p[\"label\"] for p in mask_lms]\n",
    "\n",
    "print(\"mask_idx   :\", mask_idx)\n",
    "print(\"mask_label :\", mask_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "631346bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pretrain_mask(tokens, mask_cnt, vocab_list):\n",
    "    \"\"\"\n",
    "    마스크 생성\n",
    "    :param tokens: tokens\n",
    "    :param mask_cnt: mask 개수 (전체 tokens의 15%)\n",
    "    :param vocab_list: vocab list (random token 용)\n",
    "    :return tokens: mask된 tokens\n",
    "    :return mask_idx: mask된 token의 index\n",
    "    :return mask_label: mask된 token의 원래 값\n",
    "    \"\"\"\n",
    "    # 단어 단위로 mask 하기 위해서 index 분할\n",
    "    cand_idx = []  # word 단위의 index array\n",
    "    for (i, token) in enumerate(tokens):\n",
    "        if token == \"[CLS]\" or token == \"[SEP]\":\n",
    "            continue\n",
    "        if 0 < len(cand_idx) and not token.startswith(u\"\\u2581\"):\n",
    "            cand_idx[-1].append(i)\n",
    "        else:\n",
    "            cand_idx.append([i])\n",
    "    # random mask를 위해서 순서를 섞음\n",
    "    random.shuffle(cand_idx)\n",
    "\n",
    "    mask_lms = []  # mask 된 값\n",
    "    for index_set in cand_idx:\n",
    "        if len(mask_lms) >= mask_cnt:  # 핸재 mask된 개수가 15%를 넘으면 중지\n",
    "            break\n",
    "        if len(mask_lms) + len(index_set) > mask_cnt:  # 이번에 mask할 개수를 포함해 15%를 넘으면 skip\n",
    "            continue\n",
    "        dice = random.random()  # 0..1 사이의 확률 값\n",
    "        for index in index_set:\n",
    "            masked_token = None\n",
    "            if dice < 0.8:  # 80% replace with [MASK]\n",
    "                masked_token = \"[MASK]\"\n",
    "            elif dice < 0.9: # 10% keep original\n",
    "                masked_token = tokens[index]\n",
    "            else:  # 10% random word\n",
    "                masked_token = random.choice(vocab_list)\n",
    "            mask_lms.append({\"index\": index, \"label\": tokens[index]})\n",
    "            tokens[index] = masked_token\n",
    "    # mask_lms 정렬 후 mask_idx, mask_label 추출\n",
    "    mask_lms = sorted(mask_lms, key=lambda x: x[\"index\"])\n",
    "    mask_idx = [p[\"index\"] for p in mask_lms]  # mask된 token의 index\n",
    "    mask_label = [p[\"label\"] for p in mask_lms]  # mask된 token의 원래 값\n",
    "\n",
    "    return tokens, mask_idx, mask_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7863b6ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens_org\n",
      "['[CLS]', '▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '번', '▁오', '십', '▁전', '▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에', '[SEP]', '▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러', '▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내', '▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '[SEP]'] \n",
      "\n",
      "tokens\n",
      "['[CLS]', '▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '[MASK]', '[MASK]', '[MASK]', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '번', '▁오', '십', '▁전', '▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '팹', '著', '內', '[SEP]', '▁손', '바', '닥', '[MASK]', '[MASK]', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러', '▁컬', '컬', '한', '▁목', '에', '[MASK]', '[MASK]', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내', '[MASK]', '[MASK]', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '[SEP]'] \n",
      "\n",
      "mask_idx   : [21, 22, 23, 48, 49, 50, 55, 56, 62, 63, 69, 70, 86, 87]\n",
      "mask_label : ['▁많', '아', '▁첫', '▁서', '푼', '에', '▁위', '엔', '▁흘', '러', '▁모', '주', '▁생각', '에']\n"
     ]
    }
   ],
   "source": [
    "# tokens가 mask되므로 재 실행을 위해서 넣어줌 (테스트용)\n",
    "tokens = copy.deepcopy(tokens_org)\n",
    "\n",
    "tokens, mask_idx, mask_label = create_pretrain_mask(tokens, mask_cnt, vocab_list)\n",
    "\n",
    "print(\"tokens_org\")\n",
    "print(tokens_org, \"\\n\")\n",
    "print(\"tokens\")\n",
    "print(tokens, \"\\n\")\n",
    "\n",
    "print(\"mask_idx   :\", mask_idx)\n",
    "print(\"mask_label :\", mask_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2c3e8fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"\"\"추적추적 비가 내리는 날이었어\n",
    "그날은 왠지 손님이 많아\n",
    "첫 번에 삼십 전 둘째 번 오십 전\n",
    "오랜만에 받아보는 십 전짜리 백통화 서푼에\n",
    "손바닥 위엔 기쁨의 눈물이 흘러\n",
    "컬컬한 목에 모주 한잔을 적셔\n",
    "몇 달 포 전부터 콜록거리는 아내\n",
    "생각에 그토록 먹고 싶다던\n",
    "설렁탕 한 그릇을 이제는 살 수 있어\n",
    "집으로 돌아가는 길 난 문득 떠올라\n",
    "아내의 목소리가 거칠어만 가는 희박한 숨소리가\n",
    "오늘은 왠지 나가지 말라던 내 옆에 있어 달라던\n",
    "그리도 나가고 싶으면 일찍이라도 들어와 달라던\n",
    "아내의 간절한 목소리가 들려와\n",
    "나를 원망하듯 비는 점점 거세져\n",
    "싸늘히 식어가는 아내가 떠올라 걱정은 더해져\n",
    "난 몰라 오늘은 운수 좋은 날\n",
    "난 맨날 이렇게 살 수 있으면 얼마나 좋을까\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ef6701df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어'],\n",
       " ['▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아'],\n",
       " ['▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '▁번', '▁오', '십', '▁전']]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 줄 단위로 tokenize\n",
    "doc = [vocab.encode_as_pieces(line) for line in string.split(\"\\n\")]\n",
    "doc[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "572c1d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최대 길이\n",
    "n_test_seq = 64\n",
    "# 최소 길이\n",
    "min_seq = 8\n",
    "# [CLS], tokens_a, [SEB], tokens_b, [SEP]\n",
    "max_seq = n_test_seq - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0f488ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_chunk: 5 62 [['▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어'], ['▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아'], ['▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '▁번', '▁오', '십', '▁전'], ['▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에'], ['▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러']]\n",
      "tokens_a: 50 ['▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '▁번', '▁오', '십', '▁전', '▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에']\n",
      "tokens_b: 12 ['▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러']\n",
      "\n",
      "current_chunk: 6 71 [['▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔'], ['▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내'], ['▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던'], ['▁설', '렁', '탕', '▁한', '▁그', '릇', '을', '▁이', '제는', '▁살', '▁수', '▁있어'], ['▁집', '으로', '▁돌아', '가는', '▁길', '▁난', '▁문', '득', '▁떠', '올', '라'], ['▁아내', '의', '▁목', '소', '리가', '▁거', '칠', '어', '만', '▁가는', '▁희', '박', '한', '▁숨', '소', '리가']]\n",
      "tokens_a: 55 ['▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내', '▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '▁설', '렁', '탕', '▁한', '▁그', '릇', '을', '▁이', '제는', '▁살', '▁수', '▁있어', '▁집', '으로', '▁돌아', '가는', '▁길', '▁난', '▁문', '득', '▁떠', '올', '라']\n",
      "tokens_b: 16 ['▁아내', '의', '▁목', '소', '리가', '▁거', '칠', '어', '만', '▁가는', '▁희', '박', '한', '▁숨', '소', '리가']\n",
      "\n",
      "current_chunk: 5 73 [['▁오늘', '은', '▁', '왠', '지', '▁나', '가지', '▁말', '라', '던', '▁내', '▁옆', '에', '▁있어', '▁달', '라', '던'], ['▁그리', '도', '▁나가', '고', '▁싶', '으면', '▁일', '찍', '이라', '도', '▁들어', '와', '▁달', '라', '던'], ['▁아내', '의', '▁간', '절', '한', '▁목', '소', '리가', '▁들', '려', '와'], ['▁나', '를', '▁원', '망', '하', '듯', '▁비', '는', '▁점', '점', '▁거', '세', '져'], ['▁싸', '늘', '히', '▁식', '어', '가는', '▁아내', '가', '▁떠', '올', '라', '▁', '걱', '정은', '▁더', '해', '져']]\n",
      "tokens_a: 56 ['▁오늘', '은', '▁', '왠', '지', '▁나', '가지', '▁말', '라', '던', '▁내', '▁옆', '에', '▁있어', '▁달', '라', '던', '▁그리', '도', '▁나가', '고', '▁싶', '으면', '▁일', '찍', '이라', '도', '▁들어', '와', '▁달', '라', '던', '▁아내', '의', '▁간', '절', '한', '▁목', '소', '리가', '▁들', '려', '와', '▁나', '를', '▁원', '망', '하', '듯', '▁비', '는', '▁점', '점', '▁거', '세', '져']\n",
      "tokens_b: 17 ['▁싸', '늘', '히', '▁식', '어', '가는', '▁아내', '가', '▁떠', '올', '라', '▁', '걱', '정은', '▁더', '해', '져']\n",
      "\n",
      "current_chunk: 2 22 [['▁난', '▁몰', '라', '▁오늘', '은', '▁운', '수', '▁좋은', '▁날'], ['▁난', '▁맨', '날', '▁이렇게', '▁살', '▁수', '▁있', '으면', '▁얼마', '나', '▁좋', '을', '까']]\n",
      "tokens_a: 9 ['▁난', '▁몰', '라', '▁오늘', '은', '▁운', '수', '▁좋은', '▁날']\n",
      "tokens_b: 13 ['▁난', '▁맨', '날', '▁이렇게', '▁살', '▁수', '▁있', '으면', '▁얼마', '나', '▁좋', '을', '까']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "current_chunk = []  # line 단위 tokens\n",
    "current_length = 0\n",
    "for i in range(len(doc)):  # doc 전체를 loop\n",
    "    current_chunk.append(doc[i])  # line 단위로 추가\n",
    "    current_length += len(doc[i])  # current_chunk의 token 수\n",
    "    if 1 < len(current_chunk) and (i == len(doc) - 1 or current_length >= max_seq):  # 마지막 줄 이거나 길이가 max_seq 이상 인 경우, 학습 데이터를 만듭니다. \n",
    "        print(\"current_chunk:\", len(current_chunk), current_length, current_chunk)\n",
    "\n",
    "        #######################################\n",
    "        # token a\n",
    "        a_end = 1\n",
    "        if 1 < len(current_chunk):\n",
    "            a_end = random.randrange(1, len(current_chunk))\n",
    "        tokens_a = []\n",
    "        for j in range(a_end):\n",
    "            tokens_a.extend(current_chunk[j])\n",
    "        # token b\n",
    "        tokens_b = []\n",
    "        for j in range(a_end, len(current_chunk)):\n",
    "            tokens_b.extend(current_chunk[j])\n",
    "          \n",
    "        print(\"tokens_a:\", len(tokens_a), tokens_a)\n",
    "        print(\"tokens_b:\", len(tokens_b), tokens_b)\n",
    "        #######################################\n",
    "        print()\n",
    "\n",
    "        current_chunk = []\n",
    "        current_length = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d069b9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_tokens(tokens_a, tokens_b, max_seq):\n",
    "    \"\"\"\n",
    "    tokens_a, tokens_b의 길이를 줄임 최대 길이: max_seq\n",
    "    :param tokens_a: tokens A\n",
    "    :param tokens_b: tokens B\n",
    "    :param max_seq: 두 tokens 길이의 최대 값\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_seq:\n",
    "            break\n",
    "\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            del tokens_a[0]\n",
    "        else:\n",
    "            tokens_b.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a9e82eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_chunk: 5 62 [['▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어'], ['▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아'], ['▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '▁번', '▁오', '십', '▁전'], ['▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에'], ['▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러']]\n",
      "is_next: 0\n",
      "tokens_a: 28 ['▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에', '▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러']\n",
      "tokens_b: 33 ['▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '▁번', '▁오', '십']\n",
      "\n",
      "current_chunk: 6 71 [['▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔'], ['▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내'], ['▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던'], ['▁설', '렁', '탕', '▁한', '▁그', '릇', '을', '▁이', '제는', '▁살', '▁수', '▁있어'], ['▁집', '으로', '▁돌아', '가는', '▁길', '▁난', '▁문', '득', '▁떠', '올', '라'], ['▁아내', '의', '▁목', '소', '리가', '▁거', '칠', '어', '만', '▁가는', '▁희', '박', '한', '▁숨', '소', '리가']]\n",
      "is_next: 0\n",
      "tokens_a: 39 ['▁설', '렁', '탕', '▁한', '▁그', '릇', '을', '▁이', '제는', '▁살', '▁수', '▁있어', '▁집', '으로', '▁돌아', '가는', '▁길', '▁난', '▁문', '득', '▁떠', '올', '라', '▁아내', '의', '▁목', '소', '리가', '▁거', '칠', '어', '만', '▁가는', '▁희', '박', '한', '▁숨', '소', '리가']\n",
      "tokens_b: 22 ['▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내']\n",
      "\n",
      "current_chunk: 5 73 [['▁오늘', '은', '▁', '왠', '지', '▁나', '가지', '▁말', '라', '던', '▁내', '▁옆', '에', '▁있어', '▁달', '라', '던'], ['▁그리', '도', '▁나가', '고', '▁싶', '으면', '▁일', '찍', '이라', '도', '▁들어', '와', '▁달', '라', '던'], ['▁아내', '의', '▁간', '절', '한', '▁목', '소', '리가', '▁들', '려', '와'], ['▁나', '를', '▁원', '망', '하', '듯', '▁비', '는', '▁점', '점', '▁거', '세', '져'], ['▁싸', '늘', '히', '▁식', '어', '가는', '▁아내', '가', '▁떠', '올', '라', '▁', '걱', '정은', '▁더', '해', '져']]\n",
      "is_next: 1\n",
      "tokens_a: 17 ['▁오늘', '은', '▁', '왠', '지', '▁나', '가지', '▁말', '라', '던', '▁내', '▁옆', '에', '▁있어', '▁달', '라', '던']\n",
      "tokens_b: 44 ['▁그리', '도', '▁나가', '고', '▁싶', '으면', '▁일', '찍', '이라', '도', '▁들어', '와', '▁달', '라', '던', '▁아내', '의', '▁간', '절', '한', '▁목', '소', '리가', '▁들', '려', '와', '▁나', '를', '▁원', '망', '하', '듯', '▁비', '는', '▁점', '점', '▁거', '세', '져', '▁싸', '늘', '히', '▁식', '어']\n",
      "\n",
      "current_chunk: 2 22 [['▁난', '▁몰', '라', '▁오늘', '은', '▁운', '수', '▁좋은', '▁날'], ['▁난', '▁맨', '날', '▁이렇게', '▁살', '▁수', '▁있', '으면', '▁얼마', '나', '▁좋', '을', '까']]\n",
      "is_next: 1\n",
      "tokens_a: 9 ['▁난', '▁몰', '라', '▁오늘', '은', '▁운', '수', '▁좋은', '▁날']\n",
      "tokens_b: 13 ['▁난', '▁맨', '날', '▁이렇게', '▁살', '▁수', '▁있', '으면', '▁얼마', '나', '▁좋', '을', '까']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "current_chunk = []  # line 단위 tokens\n",
    "current_length = 0\n",
    "for i in range(len(doc)):  # doc 전체를 loop\n",
    "    current_chunk.append(doc[i])  # line 단위로 추가\n",
    "    current_length += len(doc[i])  # current_chunk의 token 수\n",
    "    if 1 < len(current_chunk) and (i == len(doc) - 1 or current_length >= max_seq):  # 마지막 줄 이거나 길이가 max_seq 이상 인 경우\n",
    "        print(\"current_chunk:\", len(current_chunk), current_length, current_chunk)\n",
    "\n",
    "        # token a\n",
    "        a_end = 1\n",
    "        if 1 < len(current_chunk):\n",
    "            a_end = random.randrange(1, len(current_chunk))\n",
    "        tokens_a = []\n",
    "        for j in range(a_end):\n",
    "            tokens_a.extend(current_chunk[j])\n",
    "        # token b\n",
    "        tokens_b = []\n",
    "        for j in range(a_end, len(current_chunk)):\n",
    "            tokens_b.extend(current_chunk[j])\n",
    "\n",
    "        #######################################\n",
    "        if random.random() < 0.5:  # 50% 확률로 swap\n",
    "            is_next = 0     #False\n",
    "            tokens_t = tokens_a\n",
    "            tokens_a = tokens_b\n",
    "            tokens_b = tokens_t\n",
    "        else:\n",
    "            is_next = 1    #True\n",
    "        # max_seq 보다 큰 경우 길이 조절\n",
    "        trim_tokens(tokens_a, tokens_b, max_seq)\n",
    "        assert 0 < len(tokens_a)\n",
    "        assert 0 < len(tokens_b)\n",
    "\n",
    "        print(\"is_next:\", is_next)\n",
    "        print(\"tokens_a:\", len(tokens_a), tokens_a)\n",
    "        print(\"tokens_b:\", len(tokens_b), tokens_b)\n",
    "        #######################################\n",
    "        print()\n",
    "\n",
    "        current_chunk = []\n",
    "        current_length = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b3b7d438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_chunk: 5 62 [['▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어'], ['▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아'], ['▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '▁번', '▁오', '십', '▁전'], ['▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에'], ['▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러']]\n",
      "is_next: 0\n",
      "tokens_a: 12 ['▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러']\n",
      "tokens_b: 49 ['▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '▁번', '▁오', '십', '▁전', '▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼']\n",
      "tokens: 64 ['[CLS]', '▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러', '[SEP]', '▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '▁번', '▁오', '십', '▁전', '▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '[SEP]']\n",
      "segment: 64 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "masked tokens: 64 ['[CLS]', '▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러', '[SEP]', '▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '[MASK]', '[MASK]', '[MASK]', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '[MASK]', '▁번', '에', '▁삼', '십', '가로', '[MASK]', '[MASK]', '▁번', '[MASK]', '[MASK]', '▁전', '▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '[SEP]']\n",
      "masked index: 9 [25, 26, 27, 36, 41, 42, 43, 45, 46]\n",
      "masked label: 9 ['▁그', '날', '은', '▁첫', '▁전', '▁둘', '째', '▁오', '십']\n",
      "\n",
      "current_chunk: 6 71 [['▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔'], ['▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내'], ['▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던'], ['▁설', '렁', '탕', '▁한', '▁그', '릇', '을', '▁이', '제는', '▁살', '▁수', '▁있어'], ['▁집', '으로', '▁돌아', '가는', '▁길', '▁난', '▁문', '득', '▁떠', '올', '라'], ['▁아내', '의', '▁목', '소', '리가', '▁거', '칠', '어', '만', '▁가는', '▁희', '박', '한', '▁숨', '소', '리가']]\n",
      "is_next: 1\n",
      "tokens_a: 12 ['▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔']\n",
      "tokens_b: 49 ['▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내', '▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '▁설', '렁', '탕', '▁한', '▁그', '릇', '을', '▁이', '제는', '▁살', '▁수', '▁있어', '▁집', '으로', '▁돌아', '가는', '▁길', '▁난', '▁문', '득', '▁떠', '올', '라', '▁아내', '의', '▁목', '소', '리가', '▁거']\n",
      "tokens: 64 ['[CLS]', '▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '[SEP]', '▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내', '▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '▁설', '렁', '탕', '▁한', '▁그', '릇', '을', '▁이', '제는', '▁살', '▁수', '▁있어', '▁집', '으로', '▁돌아', '가는', '▁길', '▁난', '▁문', '득', '▁떠', '올', '라', '▁아내', '의', '▁목', '소', '리가', '▁거', '[SEP]']\n",
      "segment: 64 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "masked tokens: 64 ['[CLS]', '▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '暴', '°', '[SEP]', '▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내', '▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '▁설', '렁', '탕', '▁한', '▁그', '릇', '을', '▁이', '제는', '[MASK]', '▁수', '▁있어', '▁집', '으로', '▁돌아', '가는', '[MASK]', '▁난', '▁문', '득', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁목', '소', '리가', '▁거', '[SEP]']\n",
      "masked index: 9 [11, 12, 43, 50, 54, 55, 56, 57, 58]\n",
      "masked label: 9 ['▁적', '셔', '▁살', '▁길', '▁떠', '올', '라', '▁아내', '의']\n",
      "\n",
      "current_chunk: 5 73 [['▁오늘', '은', '▁', '왠', '지', '▁나', '가지', '▁말', '라', '던', '▁내', '▁옆', '에', '▁있어', '▁달', '라', '던'], ['▁그리', '도', '▁나가', '고', '▁싶', '으면', '▁일', '찍', '이라', '도', '▁들어', '와', '▁달', '라', '던'], ['▁아내', '의', '▁간', '절', '한', '▁목', '소', '리가', '▁들', '려', '와'], ['▁나', '를', '▁원', '망', '하', '듯', '▁비', '는', '▁점', '점', '▁거', '세', '져'], ['▁싸', '늘', '히', '▁식', '어', '가는', '▁아내', '가', '▁떠', '올', '라', '▁', '걱', '정은', '▁더', '해', '져']]\n",
      "is_next: 1\n",
      "tokens_a: 17 ['▁오늘', '은', '▁', '왠', '지', '▁나', '가지', '▁말', '라', '던', '▁내', '▁옆', '에', '▁있어', '▁달', '라', '던']\n",
      "tokens_b: 44 ['▁그리', '도', '▁나가', '고', '▁싶', '으면', '▁일', '찍', '이라', '도', '▁들어', '와', '▁달', '라', '던', '▁아내', '의', '▁간', '절', '한', '▁목', '소', '리가', '▁들', '려', '와', '▁나', '를', '▁원', '망', '하', '듯', '▁비', '는', '▁점', '점', '▁거', '세', '져', '▁싸', '늘', '히', '▁식', '어']\n",
      "tokens: 64 ['[CLS]', '▁오늘', '은', '▁', '왠', '지', '▁나', '가지', '▁말', '라', '던', '▁내', '▁옆', '에', '▁있어', '▁달', '라', '던', '[SEP]', '▁그리', '도', '▁나가', '고', '▁싶', '으면', '▁일', '찍', '이라', '도', '▁들어', '와', '▁달', '라', '던', '▁아내', '의', '▁간', '절', '한', '▁목', '소', '리가', '▁들', '려', '와', '▁나', '를', '▁원', '망', '하', '듯', '▁비', '는', '▁점', '점', '▁거', '세', '져', '▁싸', '늘', '히', '▁식', '어', '[SEP]']\n",
      "segment: 64 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "masked tokens: 64 ['[CLS]', '[MASK]', '[MASK]', '▁', '왠', '지', '▁나', '가지', '▁말', '라', '던', '▁내', '▁옆', '에', '▁있어', '[MASK]', '[MASK]', '[MASK]', '[SEP]', '▁그리', '도', '▁나가', '고', '▁싶', '으면', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁들어', '와', '▁달', '라', '던', '▁아내', '의', '▁간', '절', '한', '▁목', '소', '리가', '▁들', '려', '와', '▁나', '를', '▁원', '망', '하', '듯', '▁비', '는', '▁점', '점', '▁거', '세', '져', '▁싸', '늘', '히', '▁식', '어', '[SEP]']\n",
      "masked index: 9 [1, 2, 15, 16, 17, 25, 26, 27, 28]\n",
      "masked label: 9 ['▁오늘', '은', '▁달', '라', '던', '▁일', '찍', '이라', '도']\n",
      "\n",
      "current_chunk: 2 22 [['▁난', '▁몰', '라', '▁오늘', '은', '▁운', '수', '▁좋은', '▁날'], ['▁난', '▁맨', '날', '▁이렇게', '▁살', '▁수', '▁있', '으면', '▁얼마', '나', '▁좋', '을', '까']]\n",
      "is_next: 0\n",
      "tokens_a: 13 ['▁난', '▁맨', '날', '▁이렇게', '▁살', '▁수', '▁있', '으면', '▁얼마', '나', '▁좋', '을', '까']\n",
      "tokens_b: 9 ['▁난', '▁몰', '라', '▁오늘', '은', '▁운', '수', '▁좋은', '▁날']\n",
      "tokens: 25 ['[CLS]', '▁난', '▁맨', '날', '▁이렇게', '▁살', '▁수', '▁있', '으면', '▁얼마', '나', '▁좋', '을', '까', '[SEP]', '▁난', '▁몰', '라', '▁오늘', '은', '▁운', '수', '▁좋은', '▁날', '[SEP]']\n",
      "segment: 25 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "masked tokens: 25 ['[CLS]', '▁난', '▁맨', '날', '▁이렇게', '▁살', '▁수', '▁있', '으면', '▁얼마', '나', '▁좋', '을', '까', '[SEP]', '▁난', '▁몰', '라', '▁오늘', '은', '▁운', '수', '▁좋은', '▁날', '[SEP]']\n",
      "masked index: 3 [7, 8, 15]\n",
      "masked label: 3 ['▁있', '으면', '▁난']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "instances = []\n",
    "current_chunk = []  # line 단위 tokens\n",
    "current_length = 0\n",
    "for i in range(len(doc)):  # doc 전체를 loop\n",
    "    current_chunk.append(doc[i])  # line 단위로 추가\n",
    "    current_length += len(doc[i])  # current_chunk의 token 수\n",
    "    if 1 < len(current_chunk) and (i == len(doc) - 1 or current_length >= max_seq):  # 마지막 줄 이거나 길이가 max_seq 이상 인 경우\n",
    "        print(\"current_chunk:\", len(current_chunk), current_length, current_chunk)\n",
    "\n",
    "        # token a\n",
    "        a_end = 1\n",
    "        if 1 < len(current_chunk):\n",
    "            a_end = random.randrange(1, len(current_chunk))\n",
    "        tokens_a = []\n",
    "        for j in range(a_end):\n",
    "            tokens_a.extend(current_chunk[j])\n",
    "        # token b\n",
    "        tokens_b = []\n",
    "        for j in range(a_end, len(current_chunk)):\n",
    "            tokens_b.extend(current_chunk[j])\n",
    "\n",
    "        if random.random() < 0.5:  # 50% 확률로 swap\n",
    "            is_next = 0    # False\n",
    "            tokens_t = tokens_a\n",
    "            tokens_a = tokens_b\n",
    "            tokens_b = tokens_t\n",
    "        else:\n",
    "            is_next = 1   # True\n",
    "        # max_seq 보다 큰 경우 길이 조절\n",
    "        trim_tokens(tokens_a, tokens_b, max_seq)\n",
    "        assert 0 < len(tokens_a)\n",
    "        assert 0 < len(tokens_b)\n",
    "\n",
    "        print(\"is_next:\", is_next)\n",
    "        print(\"tokens_a:\", len(tokens_a), tokens_a)\n",
    "        print(\"tokens_b:\", len(tokens_b), tokens_b)\n",
    "        #######################################\n",
    "\n",
    "        # tokens & segment 생성\n",
    "        tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"] + tokens_b + [\"[SEP]\"]\n",
    "        segment = [0] * (len(tokens_a) + 2) + [1] * (len(tokens_b) + 1)\n",
    "        print(\"tokens:\", len(tokens), tokens)\n",
    "        print(\"segment:\", len(segment), segment)\n",
    "        \n",
    "        # mask\n",
    "        tokens, mask_idx, mask_label = create_pretrain_mask(tokens, int((len(tokens) - 3) * 0.15), vocab_list)\n",
    "        print(\"masked tokens:\", len(tokens), tokens)\n",
    "        print(\"masked index:\", len(mask_idx), mask_idx)\n",
    "        print(\"masked label:\", len(mask_label), mask_label)\n",
    "\n",
    "        instance = {\n",
    "            \"tokens\": tokens,\n",
    "            \"segment\": segment,\n",
    "            \"is_next\": is_next,\n",
    "            \"mask_idx\": mask_idx,\n",
    "            \"mask_label\": mask_label\n",
    "        }\n",
    "        instances.append(instance)\n",
    "        #######################################\n",
    "        print()\n",
    "\n",
    "        current_chunk = []\n",
    "        current_length = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2e42e603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['[CLS]', '▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러', '[SEP]', '▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '[MASK]', '[MASK]', '[MASK]', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '[MASK]', '▁번', '에', '▁삼', '십', '가로', '[MASK]', '[MASK]', '▁번', '[MASK]', '[MASK]', '▁전', '▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [25, 26, 27, 36, 41, 42, 43, 45, 46], 'mask_label': ['▁그', '날', '은', '▁첫', '▁전', '▁둘', '째', '▁오', '십']}\n",
      "{'tokens': ['[CLS]', '▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '暴', '°', '[SEP]', '▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내', '▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '▁설', '렁', '탕', '▁한', '▁그', '릇', '을', '▁이', '제는', '[MASK]', '▁수', '▁있어', '▁집', '으로', '▁돌아', '가는', '[MASK]', '▁난', '▁문', '득', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁목', '소', '리가', '▁거', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [11, 12, 43, 50, 54, 55, 56, 57, 58], 'mask_label': ['▁적', '셔', '▁살', '▁길', '▁떠', '올', '라', '▁아내', '의']}\n",
      "{'tokens': ['[CLS]', '[MASK]', '[MASK]', '▁', '왠', '지', '▁나', '가지', '▁말', '라', '던', '▁내', '▁옆', '에', '▁있어', '[MASK]', '[MASK]', '[MASK]', '[SEP]', '▁그리', '도', '▁나가', '고', '▁싶', '으면', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁들어', '와', '▁달', '라', '던', '▁아내', '의', '▁간', '절', '한', '▁목', '소', '리가', '▁들', '려', '와', '▁나', '를', '▁원', '망', '하', '듯', '▁비', '는', '▁점', '점', '▁거', '세', '져', '▁싸', '늘', '히', '▁식', '어', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [1, 2, 15, 16, 17, 25, 26, 27, 28], 'mask_label': ['▁오늘', '은', '▁달', '라', '던', '▁일', '찍', '이라', '도']}\n",
      "{'tokens': ['[CLS]', '▁난', '▁맨', '날', '▁이렇게', '▁살', '▁수', '▁있', '으면', '▁얼마', '나', '▁좋', '을', '까', '[SEP]', '▁난', '▁몰', '라', '▁오늘', '은', '▁운', '수', '▁좋은', '▁날', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [7, 8, 15], 'mask_label': ['▁있', '으면', '▁난']}\n"
     ]
    }
   ],
   "source": [
    "# 최종 데이터셋 결과 확인\n",
    "for instance in instances:\n",
    "    print(instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b6458baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pretrain_instances(vocab, doc, n_seq, mask_prob, vocab_list):\n",
    "    \"\"\"\n",
    "    doc별 pretrain 데이터 생성\n",
    "    \"\"\"\n",
    "    # for CLS], [SEP], [SEP]\n",
    "    max_seq = n_seq - 3\n",
    "\n",
    "    instances = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    for i in range(len(doc)):\n",
    "        current_chunk.append(doc[i])  # line\n",
    "        current_length += len(doc[i])\n",
    "        if 1 < len(current_chunk) and (i == len(doc) - 1 or current_length >= max_seq):\n",
    "            # token a\n",
    "            a_end = 1\n",
    "            if 1 < len(current_chunk):\n",
    "                a_end = random.randrange(1, len(current_chunk))\n",
    "            tokens_a = []\n",
    "            for j in range(a_end):\n",
    "                tokens_a.extend(current_chunk[j])\n",
    "            # token b\n",
    "            tokens_b = []\n",
    "            for j in range(a_end, len(current_chunk)):\n",
    "                tokens_b.extend(current_chunk[j])\n",
    "\n",
    "            if random.random() < 0.5:  # 50% 확률로 swap\n",
    "                is_next = 0\n",
    "                tokens_t = tokens_a\n",
    "                tokens_a = tokens_b\n",
    "                tokens_b = tokens_t\n",
    "            else:\n",
    "                is_next = 1\n",
    "            # max_seq 보다 큰 경우 길이 조절\n",
    "            trim_tokens(tokens_a, tokens_b, max_seq)\n",
    "            assert 0 < len(tokens_a)\n",
    "            assert 0 < len(tokens_b)\n",
    "            # tokens & aegment 생성\n",
    "            tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"] + tokens_b + [\"[SEP]\"]\n",
    "            segment = [0] * (len(tokens_a) + 2) + [1] * (len(tokens_b) + 1)\n",
    "            # mask\n",
    "            tokens, mask_idx, mask_label = create_pretrain_mask(tokens, int((len(tokens) - 3) * mask_prob), vocab_list)\n",
    "\n",
    "            instance = {\n",
    "                \"tokens\": tokens,\n",
    "                \"segment\": segment,\n",
    "                \"is_next\": is_next,\n",
    "                \"mask_idx\": mask_idx,\n",
    "                \"mask_label\": mask_label\n",
    "            }\n",
    "            instances.append(instance)\n",
    "\n",
    "            current_chunk = []\n",
    "            current_length = 0\n",
    "    return instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7cbb1e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['[CLS]', '▁번', '에', '▁삼', '십', '[MASK]', '▁둘', '째', 'え', '▁오', '십', '▁전', '▁오', '랜', '만에', '[MASK]', '[MASK]', '[MASK]', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에', '▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[SEP]', '▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [5, 8, 15, 16, 17, 36, 37, 38, 39], 'mask_label': ['▁전', '▁번', '▁받아', '보', '는', '▁눈', '물이', '▁흘', '러']}\n",
      "{'tokens': ['[CLS]', '[MASK]', '[MASK]', '▁몇', '▁달', '[MASK]', '▁전', '부터', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁아내', '▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '▁설', '렁', '탕', '▁한', '▁그', '릇', '을', '▁이', '제는', '[MASK]', '▁수', '▁있어', '▁집', '으로', '▁돌아', '가는', '[MASK]', '▁난', '▁문', '득', '▁떠', '올', '라', '[SEP]', '▁아내', '의', '▁목', '소', '리가', '▁거', '칠', '어', '만', '▁가는', '▁희', '박', '한', '▁숨', '소', '리가', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [1, 2, 5, 8, 9, 10, 11, 32, 39], 'mask_label': ['▁적', '셔', '▁포', '▁콜', '록', '거', '리는', '▁살', '▁길']}\n",
      "{'tokens': ['[CLS]', '에', '▁있어', '▁달', '라', '던', '▁그리', '도', '[MASK]', '[MASK]', '▁싶', '으면', '▁일', '찍', '이라', '도', '▁들어', '와', '▁달', '라', '던', '▁아내', '의', '▁간', '절', '한', '▁목', '소', '리가', '▁들', '려', '와', '[SEP]', '▁나', '를', '▁원', '망', '하', '듯', '▁비', '는', '▁점', '점', '썹', '숙', '촐', '▁싸', '늘', '히', '▁식', '어', '가는', '▁아내', '가', '▁떠', '올', '라', '[MASK]', '[MASK]', '[MASK]', '▁더', '해', '져', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [2, 8, 9, 43, 44, 45, 57, 58, 59], 'mask_label': ['▁있어', '▁나가', '고', '▁거', '세', '져', '▁', '걱', '정은']}\n",
      "{'tokens': ['[CLS]', '▁난', '[MASK]', '[MASK]', '▁이렇게', '▁살', '[MASK]', '▁있', '으면', '▁얼마', '나', '▁좋', '을', '까', '[SEP]', '▁난', '▁몰', '라', '▁오늘', '은', '▁운', '수', '▁좋은', '▁날', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [2, 3, 6], 'mask_label': ['▁맨', '날', '▁수']}\n"
     ]
    }
   ],
   "source": [
    "instances = create_pretrain_instances(vocab, doc, n_test_seq, 0.15, vocab_list)\n",
    "\n",
    "# 최종 데이터셋 결과 확인\n",
    "for instance in instances:\n",
    "    print(instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "131256c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3957761"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_file = os.getenv('HOME')+'/aiffel/bert_pretrain/data/kowiki.txt'\n",
    "\n",
    "# line count 확인\n",
    "total = 0\n",
    "with open(corpus_file, 'r') as in_f:\n",
    "    for line in in_f:\n",
    "        total += 1\n",
    "\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e2d6b9f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3957761"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7cb4e1e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2c7f809153d4390913a639da8e1bd9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3957761 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 lines : ['▁지', '미', '▁카', '터']\n",
      "['▁제임스', '▁얼', '▁\"', '지', '미', '\"', '▁카', '터', '▁주', '니어', '(,', '▁192', '4', '년', '▁10', '월', '▁1', '일', '▁~', '▁)', '는', '▁민주', '당', '▁출신', '▁미국', '▁3', '9', '번째', '▁대통령', '▁(19', '7', '7', '년', '▁~', '▁1981', '년', ')', '이다', '.']\n",
      "['▁그는', '▁2002', '년', '▁말', '▁인', '권', '과', '▁중', '재', '▁역할', '에', '▁대한', '▁공', '로를', '▁인정', '받아', '▁노', '벨', '▁평화', '상을', '▁받', '게', '▁되었다', '.']\n",
      "\n",
      "['▁수학']\n",
      "['▁수학', '(', '數', '學', ',', '▁)', '은', '▁양', ',', '▁구조', ',', '▁공간', ',', '▁변화', ',', '▁미', '적', '분', '▁등의', '▁개념', '을', '▁다루', '는', '▁학', '문', '이다', '.', '▁현대', '▁수학', '은', '▁형식', '▁논', '리를', '▁이용', '해서', '▁공', '리로', '▁구성된', '▁추', '상', '적', '▁구조를', '▁연구', '하는', '▁학', '문', '으로', '▁여겨', '지', '기도', '▁한다', '.', '▁수학', '은', '▁그', '▁구조', '와', '▁발전', '▁과정', '에서는', '▁자연', '과학', '에', '▁속하는', '▁물리', '학을', '▁비롯한', '▁다른', '▁학', '문', '들과', '▁깊', '은', '▁연', '관을', '▁맺', '고', '▁있다', '.', '▁하지만', ',', '▁어느', '▁과학', '의', '▁분야', '들과', '는', '▁달리', ',', '▁자연', '계에서', '▁관측', '되지', '▁않는', '▁개념', '들에', '▁대해서', '까지', '▁이론', '을', '▁일반', '화', '▁및', '▁추', '상', '화', '시', '킬', '▁수', '▁있다는', '▁차', '이가', '▁있다고', '▁한다', '.', '▁수', '학자', '들은', '▁그러', '한', '▁개념', '들에', '▁대해서', '▁추', '측', '을', '▁하고', ',', '▁적', '절', '하게', '▁선택', '된', '▁정의', '와', '▁공', '리', '로부터', '의', '▁엄', '밀', '한', '▁연', '역을', '▁통해', '서', '▁추', '측', '들의', '▁진', '위를', '▁파', '악', '한다', '.']\n",
      "['▁수', '학의', '▁기초', '를', '▁확', '실', '히', '▁세', '우', '기', '▁위해', ',', '▁수', '리', '논', '리', '학과', '▁집합', '론', '이', '▁발전', '하였고', ',', '▁이와', '▁더불어', '▁범', '주', '론', '이', '▁최근', '에도', '▁발전', '되고', '▁있다', '.', '▁“', '근', '본', '▁위', '기', '”', '라는', '▁말', '은', '▁대', '략', '▁19', '00', '년', '에서', '▁1930', '년', '▁사이에', '▁일어난', ',', '▁수', '학의', '▁엄', '밀', '한', '▁기초', '에', '▁대한', '▁탐', '구를', '▁상징', '적으로', '▁보여', '주는', '▁말이다', '.', '▁수', '학의', '▁엄', '밀', '한', '▁기초', '에', '▁대한', '▁몇', '▁가지', '▁의견', '▁불', '일', '치는', '▁오늘날', '에도', '▁계속', '되고', '▁있다', '.', '▁수', '학의', '▁기초', '에', '▁대한', '▁위', '기는', '▁그', '▁당시', '▁수많은', '▁논', '쟁', '에', '▁의해', '▁촉', '발', '되었으며', ',', '▁그', '▁논', '쟁', '에는', '▁칸', '토', '어의', '▁집합', '론', '과', '▁브라', '우', '어', '-', '힐', '베', '르트', '▁논', '쟁', '이', '▁포함', '되었다', '.']\n"
     ]
    }
   ],
   "source": [
    "# 위키가 주제별로 잘 나눠지는지 여부 확인\n",
    "count = 1\n",
    "\n",
    "text_org = []\n",
    "with open(corpus_file, 'r') as in_f:\n",
    "    doc = []  # 단락 단위로 문서 저장\n",
    "    for line in tqdm(in_f, total=total):\n",
    "        line = line.strip()\n",
    "        text_org.append(line) #데이터 확인용 저장\n",
    "        if line == \"\":  # line이 빈줄 일 경우 (새로운 단락)  \n",
    "            if 0 < len(doc):\n",
    "                if 0 < count:\n",
    "                    count -= 1\n",
    "                    print(len(doc), \"lines :\", doc[0])\n",
    "                    print(doc[1])\n",
    "                    print(doc[-1])\n",
    "                    print()\n",
    "                else:\n",
    "                    break\n",
    "                doc = []\n",
    "        else:  # 빈 줄이 아니면 doc에 저장\n",
    "            pieces = vocab.encode_as_pieces(line)    \n",
    "            if 0 < len(pieces):\n",
    "#                 doc.append(line)\n",
    "                doc.append(pieces)\n",
    "\n",
    "    if 0 < len(doc):  # 마지막에 처리되지 않은 doc가 있는 경우\n",
    "        print(doc[0])\n",
    "        print(doc[1])\n",
    "        print(doc[-1])\n",
    "        doc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "587d3705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['지미 카터',\n",
       " '제임스 얼 \"지미\" 카터 주니어(, 1924년 10월 1일 ~ )는 민주당 출신 미국 39번째 대통령 (1977년 ~ 1981년)이다.',\n",
       " '지미 카터는 조지아주 섬터 카운티 플레인스 마을에서 태어났다. 조지아 공과대학교를 졸업하였다. 그 후 해군에 들어가 전함·원자력·잠수함의 승무원으로 일하였다. 1953년 미국 해군 대위로 예편하였고 이후 땅콩·면화 등을 가꿔 많은 돈을 벌었다. 그의 별명이 \"땅콩 농부\" (Peanut Farmer)로 알려졌다.',\n",
       " '1962년 조지아 주 상원 의원 선거에서 낙선하나 그 선거가 부정선거 였음을 입증하게 되어 당선되고, 1966년 조지아 주 지사 선거에 낙선하지만 1970년 조지아 주 지사를 역임했다. 대통령이 되기 전 조지아주 상원의원을 두번 연임했으며, 1971년부터 1975년까지 조지아 지사로 근무했다. 조지아 주지사로 지내면서, 미국에 사는 흑인 등용법을 내세웠다.',\n",
       " '1976년 대통령 선거에 민주당 후보로 출마하여 도덕주의 정책으로 내세워, 포드를 누르고 당선되었다.']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(text_org))\n",
    "text_org[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "85cc29de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64697b35f688449d8ed085163a30fed4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3957761 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc: 21 instances: 10\n",
      "{'tokens': ['[CLS]', '▁논', '쟁', '에', '▁의해', '▁촉', '발', '되었으며', ',', '▁그', '▁논', '쟁', '에는', '▁칸', '토', '어의', '▁집합', '론', '과', '▁브라', '우', '어', '-', '힐', '베', '르트', '[MASK]', '[MASK]', '[MASK]', '▁포함', '되었다', '.', '[SEP]', '▁수', '학의', '[MASK]', '[MASK]', '▁확', '실', '히', '[MASK]', '[MASK]', '[MASK]', '▁위해', ',', '▁수', '리', '논', '리', '학과', '▁집합', '론', '이', '▁발전', '하였고', ',', '▁이와', '▁더불어', '▁범', '주', '론', '이', '▁최근', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [26, 27, 28, 35, 36, 40, 41, 42, 57], 'mask_label': ['▁논', '쟁', '이', '▁기초', '를', '▁세', '우', '기', '▁더불어']}\n",
      "{'tokens': ['[CLS]', '▁논', '쟁', '에', '것', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁논', '쟁', '에는', '▁칸', '토', '어의', '[MASK]', '[MASK]', '[MASK]', '▁브라', '우', '어', '-', '힐', '베', '르트', '▁논', '쟁', '이', '▁포함', '되었다', '.', '[SEP]', '▁수', '학의', '▁기초', '를', '▁확', '실', '히', '▁세', '우', '기', '▁위해', ',', '▁수', '리', '논', '리', '학과', '▁집합', '론', '이', '▁발전', '하였고', ',', '▁이와', '▁더불어', '▁범', '주', '론', '이', '▁최근', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [4, 5, 6, 7, 8, 9, 16, 17, 18], 'mask_label': ['▁의해', '▁촉', '발', '되었으며', ',', '▁그', '▁집합', '론', '과']}\n",
      "\n",
      "doc: 14 instances: 7\n",
      "{'tokens': ['[CLS]', '▁논', '쟁', '에', '▁의해', '▁촉', '발', '되었으며', ',', '▁그', '▁논', '쟁', '에는', '▁칸', '토', '어의', '▁집합', '론', '과', '▁브라', '우', '어', '-', '힐', '베', '르트', '▁논', '쟁', '이', '[MASK]', '[MASK]', '[MASK]', '[SEP]', '▁수', '학의', '▁기초', '를', '▁확', '실', '히', '▁세', '우', '기', '[MASK]', '[MASK]', '▁수', '리', '논', '리', '학과', '▁집합', '론', '이', '▁발전', '하였고', ',', '▁이와', '[MASK]', '▁범', '주', '론', '이', '▁최근', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [29, 30, 31, 43, 44, 53, 54, 55, 57], 'mask_label': ['▁포함', '되었다', '.', '▁위해', ',', '▁발전', '하였고', ',', '▁더불어']}\n",
      "{'tokens': ['[CLS]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁촉', '발', '되었으며', ',', '▁확장', '▁논', '쟁', '에는', '▁칸', '토', '어의', '▁집합', '론', '과', '▁브라', '우', '어', '-', '힐', '베', '르트', '댕', '極', '쳅', '▁포함', '되었다', '.', '[SEP]', '▁수', '학의', '▁기초', '를', '▁확', '실', '히', '▁세', '우', '기', '▁위해', ',', '▁수', '리', '논', '리', '학과', '▁집합', '론', '이', '▁발전', '하였고', ',', '▁이와', '[MASK]', '▁범', '주', '론', '이', '▁최근', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [1, 2, 3, 4, 9, 26, 27, 28, 57], 'mask_label': ['▁논', '쟁', '에', '▁의해', '▁그', '▁논', '쟁', '이', '▁더불어']}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# instance 생성 기능 확인\n",
    "count = 1\n",
    "\n",
    "with open(corpus_file, 'r') as in_f:\n",
    "    doc = []  # 단락 단위로 문서 저장\n",
    "    for line in tqdm(in_f, total=total):\n",
    "        line = line.strip()\n",
    "        if line == \"\":  # line이 빈줄 일 경우 (새로운 단락)\n",
    "            if 0 < len(doc):\n",
    "                instances = create_pretrain_instances(vocab, doc, n_test_seq, 0.15, vocab_list)\n",
    "                # save\n",
    "                print(\"doc:\", len(doc), \"instances:\", len(instances))\n",
    "                print(instances[0])\n",
    "                print(instances[-1])\n",
    "                print()\n",
    "                doc = []\n",
    "                if 0 < count:  # 테스트를 위해서 부분 처리함\n",
    "                    count -= 1\n",
    "                else:\n",
    "                    break\n",
    "        else:  # doc에 저장\n",
    "            if 0 < len(pieces):\n",
    "                doc.append(pieces)\n",
    "    if 0 < len(doc):  # 마지막에 처리되지 않은 doc가 있는 경우\n",
    "        instances = create_pretrain_instances(doc, 128)\n",
    "        # save\n",
    "        print(\"doc:\", len(doc), \"instances:\", len(instances))\n",
    "        print(instances[0])\n",
    "        print(instances[-1])\n",
    "        print()\n",
    "        doc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ab706270",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pretrain_data(vocab, in_file, out_file, n_seq, mask_prob=0.15):\n",
    "    \"\"\" pretrain 데이터 생성 \"\"\"\n",
    "    def save_pretrain_instances(out_f, doc):\n",
    "        instances = create_pretrain_instances(vocab, doc, n_seq, mask_prob, vocab_list)\n",
    "        for instance in instances:\n",
    "            out_f.write(json.dumps(instance, ensure_ascii=False))\n",
    "            out_f.write(\"\\n\")\n",
    "\n",
    "    # 특수문자 7개를 제외한 vocab_list 생성\n",
    "    vocab_list = []\n",
    "    for id in range(7, len(vocab)):\n",
    "        if not vocab.is_unknown(id):\n",
    "            vocab_list.append(vocab.id_to_piece(id))\n",
    "\n",
    "    # line count 확인\n",
    "    line_cnt = 0\n",
    "    with open(in_file, \"r\") as in_f:\n",
    "        for line in in_f:\n",
    "            line_cnt += 1\n",
    "\n",
    "    with open(in_file, \"r\") as in_f:\n",
    "        with open(out_file, \"w\") as out_f:\n",
    "            doc = []\n",
    "            for line in tqdm(in_f, total=line_cnt):\n",
    "                line = line.strip()\n",
    "                if line == \"\":  # line이 빈줄 일 경우 (새로운 단락을 의미 함)\n",
    "                    if 0 < len(doc):\n",
    "                        save_pretrain_instances(out_f, doc)\n",
    "                        doc = []\n",
    "                else:  # line이 빈줄이 아닐 경우 tokenize 해서 doc에 저장\n",
    "                    pieces = vocab.encode_as_pieces(line)\n",
    "                    if 0 < len(pieces):\n",
    "                        doc.append(pieces)\n",
    "            if 0 < len(doc):  # 마지막에 처리되지 않은 doc가 있는 경우\n",
    "                save_pretrain_instances(out_f, doc)\n",
    "                doc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f6984499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c0fd4d58ac74579ab14bd2878c48597",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3957761 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pretrain_json_path = os.getenv('HOME')+'/aiffel/bert_pretrain/data/mini_bert_pre_train.json' #mini bert관련 json파일 생성\n",
    "\n",
    "make_pretrain_data(vocab, corpus_file, pretrain_json_path, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a34af7a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       " 0,\n",
       " 0,\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_seq = 128\n",
    "# [CLS], tokens_a, [SEP], tokens_b, [SEP]\n",
    "max_seq = n_seq - 3\n",
    "\n",
    "# 만약 일반적인 Numpy Array에다 데이터를 로딩한다면 이렇게 되겠지만\n",
    "# enc_tokens = np.zeros((total, n_seq), np.int32)\n",
    "# dec_tokens = np.zeros((total, n_seq), np.int32)\n",
    "# labels_nsp = np.zeros((total,), np.int32)\n",
    "# labels_mlm = np.zeros((total, n_seq), np.int32)\n",
    "\n",
    "# np.memmap을 사용하면 메모리를 적은 메모리에서도 대용량 데이터 처리가 가능 함\n",
    "enc_tokens = np.memmap(filename='enc_tokens.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "segments = np.memmap(filename='segments.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "labels_nsp = np.memmap(filename='labels_nsp.memmap', mode='w+', dtype=np.int32, shape=(total,))\n",
    "labels_mlm = np.memmap(filename='labels_mlm.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "\n",
    "\n",
    "enc_tokens[0], enc_tokens[-1], segments[0], segments[-1], labels_nsp[0], labels_nsp[-1], labels_mlm[0], labels_mlm[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5219dfd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ca324570332490caa368a0b01fb8cac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3957761 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['[CLS]', '일', '▁~', '▁)', '는', '▁민주', '당', '[MASK]', '▁미국', '▁3', '9', '번째', '[MASK]', '▁(19', '7', '7', '년', '▁~', '▁1981', '년', ')', '이다', '.', '左', '호를', '▁카', '터', '는', '▁조지', '아', '주', '▁섬', '터', '[MASK]', '[MASK]', '▁플', '레', '인', '스', '▁마을', '에서', '[MASK]', '[MASK]', '▁조지', '아', '▁공', '과', '대학교', '를', '▁졸업', '하였다', '.', '▁그', '▁후', '▁해', '군에', '▁들어가', '▁전', '함', '·', '원', '자', '력', '·', '잠', '수', '함', '의', '[MASK]', '[MASK]', '[MASK]', '▁일', '하였다', '.', '▁195', '3', '년', '▁미국', '▁해군', '▁대', '위로', '▁예', '편', '하였고', '[MASK]', '▁땅', '콩', '·', '면', '화', '▁뮤', '▁가', '꿔', '▁많은', '▁돈', '을', '▁벌', '었다', '.', '▁그의', '[MASK]', '[MASK]', '▁\"', '땅', '콩', '▁농', '부', '\"', '▁(', 'P', 'e', 'an', 'ut', '▁F', 'ar', 'm', 'er', ')', '로', '▁알려', '졌다', '.', '[SEP]', '[MASK]', '[MASK]', '▁카', '터', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [7, 12, 23, 24, 33, 34, 41, 42, 68, 69, 70, 84, 90, 99, 100, 101, 123, 124], 'mask_label': ['▁출신', '▁대통령', '▁지', '미', '▁카운', '티', '▁태어났다', '.', '▁승', '무', '원으로', '▁이후', '▁등을', '▁그의', '▁별', '명이', '▁지', '미']}\n",
      "enc_token: [5, 3629, 203, 241, 3602, 1114, 3724, 6, 243, 49, 3632, 796, 6, 1647, 3682, 3682, 3625, 203, 3008, 3625, 3616, 16, 3599, 4984, 1395, 207, 3714, 3602, 1755, 3630, 3646, 630, 3714, 6, 6, 429, 3740, 3628, 3626, 1369, 10, 6, 6, 1755, 3630, 41, 3644, 830, 3624, 1135, 52, 3599, 13, 81, 87, 1501, 2247, 25, 3779, 3873, 3667, 3631, 3813, 3873, 4196, 3636, 3779, 3601, 6, 6, 6, 33, 52, 3599, 479, 3652, 3625, 243, 2780, 14, 1509, 168, 3877, 414, 6, 1697, 4290, 3873, 3703, 3683, 1506, 21, 5007, 399, 1927, 3607, 813, 17, 3599, 307, 6, 6, 103, 4313, 4290, 613, 3638, 3718, 98, 3878, 3656, 256, 2543, 309, 337, 3735, 181, 3616, 3603, 489, 376, 3599, 4, 6, 6, 207, 3714, 4]\n",
      "segment: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n",
      "label_nsp: 0\n",
      "label_mlm: [   0    0    0    0    0    0    0  788    0    0    0    0  663    0\n",
      "    0    0    0    0    0    0    0    0    0   18 3686    0    0    0\n",
      "    0    0    0    0    0 3565 3835    0    0    0    0    0    0 1605\n",
      " 3599    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0  249 3725\n",
      " 1232    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "  165    0    0    0    0    0  593    0    0    0    0    0    0    0\n",
      "    0  307  587  931    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0   18 3686    0\n",
      "    0    0]\n",
      "\n",
      "{'tokens': ['[CLS]', '[MASK]', '[MASK]', '▁대통령', '▁선거', '에', '▁붙', '외', '▁후보', '로', '▁출', '마', '하여', '▁도', '덕', '주의', '▁정책', '으로', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁포', '드를', '▁누', '르고', '▁당선', '되었다', '.', '[SEP]', '▁196', '2', '년', '▁조지', '아', '▁주', '▁상', '원', '▁의원', '▁선거', '에서', '▁낙', '선', '하나', '▁그', '▁선거', '가', '▁부정', '선거', '▁', '였', '음을', '▁입', '증', '하게', '[MASK]', '▁당선', '되고', ',', '▁196', '6', '년', '▁조지', '아', '▁주', '▁지', '사', '▁선거', '에', '▁낙', '선', '하지만', '▁t', '換', '▁조지', '아', '▁주', '▁지', '사를', '[MASK]', '[MASK]', '[MASK]', '▁대통령', '이', '▁되', '기', '敗', '▁조지', '아', '주', '▁상', '원의', '원을', '▁두', '번', '▁연', '임', '했으며', ',', '[MASK]', '[MASK]', '▁1975', '년까지', '▁조지', '아', '▁지', '사로', '▁근무', '했다', '.', '▁조지', '아', '▁주', '지', '사로', '▁지', '내', '면서', ',', '▁미국', '에', '[MASK]', '▁흑', '인', '▁등', '용', '법을', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [1, 2, 6, 7, 18, 19, 20, 21, 55, 72, 73, 79, 80, 81, 86, 99, 100, 121], 'mask_label': ['▁1976', '년', '▁민주', '당', '▁내', '세', '워', ',', '▁되어', '▁1970', '년', '▁역임', '했다', '.', '▁전', '▁1971', '년부터', '▁사는']}\n",
      "enc_token: [5, 6, 6, 663, 822, 3600, 1023, 3866, 958, 3603, 117, 3674, 54, 75, 4089, 238, 1421, 9, 6, 6, 6, 6, 119, 1486, 807, 2056, 2387, 43, 3599, 4, 386, 3619, 3625, 1755, 3630, 37, 76, 3667, 2378, 822, 10, 1567, 3668, 3294, 13, 822, 3608, 2386, 2163, 3596, 3671, 969, 213, 3929, 173, 6, 2387, 317, 3604, 386, 3673, 3625, 1755, 3630, 37, 18, 3620, 822, 3600, 1567, 3668, 1447, 793, 6653, 1755, 3630, 37, 18, 451, 6, 6, 6, 663, 3597, 450, 3614, 7975, 1755, 3630, 3646, 76, 955, 928, 157, 3821, 61, 3773, 530, 3604, 6, 6, 3409, 673, 1755, 3630, 18, 982, 2711, 31, 3599, 1755, 3630, 37, 3610, 982, 18, 3754, 151, 3604, 243, 3600, 6, 1733, 3628, 50, 3717, 2046, 4]\n",
      "segment: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "label_nsp: 0\n",
      "label_mlm: [   0 3306 3625    0    0    0 1114 3724    0    0    0    0    0    0\n",
      "    0    0    0    0  114 3692 3964 3604    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0  607\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0 1921 3625    0    0    0    0    0 1398   31 3599    0    0\n",
      "    0    0   25    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0 3372  523    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0 3554    0    0    0    0\n",
      "    0    0]\n",
      "\n",
      "{'tokens': ['[CLS]', '▁카', '터', '▁대통령', '은', '▁에너', '지', '▁개발', '을', '▁촉', '구', '했으나', '▁공', '화', '당의', '▁반', '대로', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[SEP]', '▁카', '터', '는', '▁이집', '트', '와', '▁이스라엘', '을', '▁조정', '하여', ',', '▁캠', '프', '▁데이', '비', '드에서', '▁안', '와', '르', '▁사', '다', '트', '▁대통령', '과', '▁메', '나', '헴', '▁베', '긴', '▁수상', '과', '▁함께', '▁중', '동', '▁평', '화를', '▁위한', '▁캠', '프', '데', '이', '비', '드', '▁협', '정을', '실을', '▁연', '▁대형', '▁그러나', '▁이것은', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁미국의', '▁유대', '인', '▁단', '체의', '▁반', '발', '을', '▁일으', '켰', '다', '.', '▁1979', '년', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁양', '국', '▁간의', '▁평화', '조', '약', '으로', '▁이끌', '어졌다', '.', '▁또한', '▁소련', '과', '▁제', '2', '차', '[MASK]', '▁무', '기', '▁제한', '▁협', '상에', '▁조', '인', '했다', '.', '▁카', '터', '는', '▁1970', '년대', '▁후반', '▁당시', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [17, 18, 19, 20, 67, 68, 69, 72, 73, 74, 75, 90, 91, 92, 93, 105, 106, 110], 'mask_label': ['▁무', '산', '되었다', '.', '▁체결', '했다', '.', '▁공', '화', '당', '과', '▁백', '악', '관', '에서', '▁소련', '과', '▁전략']}\n",
      "enc_token: [5, 207, 3714, 663, 3613, 1778, 3610, 570, 3607, 2270, 3653, 1003, 41, 3683, 1547, 141, 448, 6, 6, 6, 6, 4, 207, 3714, 3602, 2703, 3677, 3665, 3426, 3607, 3358, 54, 3604, 2432, 3721, 965, 3694, 3552, 172, 3665, 3699, 15, 3598, 3677, 663, 3644, 334, 3637, 5887, 271, 4099, 1011, 3644, 280, 35, 3658, 232, 934, 521, 2432, 3721, 3736, 3597, 3694, 3681, 617, 666, 3092, 61, 3336, 330, 1487, 6, 6, 6, 6, 679, 2670, 3628, 164, 1314, 141, 3720, 3607, 1213, 4174, 3598, 3599, 2995, 3625, 6, 6, 6, 6, 230, 3643, 2714, 2793, 3676, 3827, 9, 1435, 2521, 3599, 276, 1302, 3644, 30, 3619, 3751, 6, 107, 3614, 1956, 617, 1824, 53, 3628, 31, 3599, 207, 3714, 3602, 1921, 596, 1840, 316, 4]\n",
      "segment: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "label_nsp: 1\n",
      "label_mlm: [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0  107 3726   43 3599    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0 2525   31 3599\n",
      "    0    0   41 3683 3724 3644    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0  456 3928 3708   10    0    0    0    0\n",
      "    0    0    0    0    0    0    0 1302 3644    0    0    0 2835    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0]\n",
      "\n",
      "{'tokens': ['[CLS]', '질', '▁구', '출', '▁실패', '를', '▁이유로', '▁1980', '년', '[MASK]', '▁선거', '에서', '▁공', '화', '당의', '▁로', '널', '드', '[MASK]', '[MASK]', '▁후보', '에게', '[MASK]', '[MASK]', '[MASK]', '▁재', '선에', '▁실패', '했다', '.', '▁또한', '▁임', '기', '▁말', '기에', '▁터', '진', '▁소련', '의', '▁아', '프가', '니', '스탄', '▁침공', '[MASK]', '[MASK]', '▁인해', '▁1980', '년', '[MASK]', '[MASK]', '[MASK]', '▁반', '공', '국', '가', '들의', '▁보이', '콧', '을', '▁내', '세', '웠다', '.', '[SEP]', '▁지', '미', '▁카', '터', '는', '▁대한민국', '과의', '▁관계', '에서도', '▁중요한', '▁영향을', '▁미', '쳤', '던', '▁대통령', '▁중', '▁하나', '다', '.', '▁인', '권', '▁문제', '와', '▁주', '한', '미', '군', '▁철', '수', '▁문제', '로', '▁한때', '▁한', '미', '▁관계', '가', '▁불', '편', '하기도', '▁했다', '.', '▁1978', '년', '[MASK]', '[MASK]', '게', '▁북한', '의', '▁위협', '에', '▁대', '비', '해', '▁한', '미', '연합', '사를', '▁창설', '하면서', ',', '[MASK]', '[MASK]', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [9, 18, 19, 22, 23, 24, 44, 45, 49, 50, 51, 108, 109, 110, 111, 112, 125, 126], 'mask_label': ['▁대통령', '▁레이', '건', '▁', '져', '▁결국', '▁사건', '으로', '▁하계', '▁올림픽', '에', '▁대한민국', '에', '▁대한', '▁북한', '의', '▁1982', '년까지']}\n",
      "enc_token: [5, 3892, 73, 3771, 1579, 3624, 1827, 1640, 3625, 6, 822, 10, 41, 3683, 1547, 194, 4044, 3681, 6, 6, 958, 113, 6, 6, 6, 174, 2087, 1579, 31, 3599, 276, 273, 3614, 150, 329, 870, 3713, 1302, 3601, 26, 2986, 3733, 1323, 3232, 6, 6, 751, 1640, 3625, 6, 6, 6, 141, 3670, 3643, 3608, 247, 3052, 4805, 3607, 114, 3692, 1853, 3599, 4, 18, 3686, 207, 3714, 3602, 410, 786, 704, 643, 1165, 1063, 55, 4219, 3781, 663, 35, 324, 3598, 3599, 42, 3830, 550, 3665, 37, 3612, 3686, 3722, 380, 3636, 550, 3603, 3590, 34, 3686, 704, 3608, 128, 3877, 863, 345, 3599, 3331, 3625, 6, 6, 3669, 1876, 3601, 3038, 3600, 14, 3694, 3645, 34, 3686, 2569, 451, 3574, 421, 3604, 6, 6, 4]\n",
      "segment: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "label_nsp: 1\n",
      "label_mlm: [   0    0    0    0    0    0    0    0    0  663    0    0    0    0\n",
      "    0    0    0    0 1169 3803    0    0 3596 3944  875    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0  636    9    0    0    0 2219  779 3600    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0  410 3600   92 1876\n",
      " 3601    0    0    0    0    0    0    0    0    0    0    0    0 2760\n",
      "  673    0]\n",
      "\n",
      "{'tokens': ['[CLS]', '[MASK]', '▁뒤', '▁민주', '주의', '▁실', '현', '을', '[MASK]', '▁제', '▁3', '세', '계의', '▁선거', '▁감', '시', '▁활동', '▁및', '▁기', '니', '▁벌', '레', '에', '[MASK]', '▁드', '라', '쿤', '쿠', '르', '스', '▁질', '병', '▁방', '재', '를', '[MASK]', '▁힘', '썼', '다', '.', '[MASK]', '▁빈', '곤', '층', '▁지원', '▁활동', ',', '▁사랑', '의', '[MASK]', '[MASK]', '[MASK]', '▁운동', ',', '▁국제', '▁분', '쟁', '▁중', '재', '▁등의', '▁활동', '도', '▁했다', '.', '[SEP]', '▁1979', '년', '▁~', '▁1980', '년', '▁백', '▁정치적', '▁격', '변', '기', '▁당시의', '▁대통령', '이었던', '▁그는', '▁이에', '▁대해', '▁애', '매', '한', '▁태', '도를', '▁보', '였고', ',', '▁이는', '[MASK]', '▁대한민국', '[MASK]', '▁고', '조', '되는', '▁반', '미', '▁운동', '의', '[MASK]', '▁원', '인이', '▁', '됐다', '.', '▁10', '월', '▁26', '일', ',', '▁박', '정', '희', '▁대통령', '이', '▁김', '재', '규', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁의해', '▁살해', '된', '▁것에', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [1, 8, 23, 35, 40, 49, 50, 51, 62, 63, 70, 90, 92, 100, 119, 120, 121, 122], 'mask_label': ['한', '▁위해', '▁의한', '▁위해', '▁미국의', '▁집', '짓', '기', '▁했다', '.', '▁대한민국의', '▁후에', '▁내에서', '▁한', '▁중앙', '정보', '부', '장에']}\n",
      "enc_token: [5, 6, 339, 1114, 238, 158, 3756, 3607, 6, 30, 49, 3692, 1654, 822, 209, 3623, 375, 228, 24, 3733, 813, 3740, 3600, 6, 311, 3635, 4956, 3937, 3699, 3626, 761, 3886, 95, 3729, 3624, 6, 947, 4437, 3598, 3599, 6, 1412, 4234, 4083, 770, 375, 3604, 1424, 3601, 6, 6, 6, 887, 3604, 605, 147, 3972, 35, 3729, 507, 375, 3627, 345, 3599, 4, 2995, 3625, 203, 1640, 3625, 456, 2843, 1032, 3889, 3614, 3195, 663, 1277, 202, 695, 433, 442, 3823, 3612, 227, 701, 47, 2470, 3604, 594, 6, 410, 6, 70, 3676, 267, 141, 3686, 887, 3601, 6, 129, 828, 3596, 1027, 3599, 131, 3662, 981, 3629, 3604, 338, 3642, 4055, 663, 3597, 200, 3729, 3958, 6, 6, 6, 6, 355, 2591, 3711, 2057, 4]\n",
      "segment: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "label_nsp: 0\n",
      "label_mlm: [   0 3612    0    0    0    0    0    0  231    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0 1332    0    0    0    0\n",
      "    0    0    0    0    0    0    0  231    0    0    0    0  679    0\n",
      "    0    0    0    0    0    0    0  313 4333 3614    0    0    0    0\n",
      "    0    0    0    0    0    0  345 3599    0    0    0    0    0    0\n",
      "  447    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0 1140    0 3428    0    0    0    0    0\n",
      "    0    0   34    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0  782 2275 3638 1312    0    0    0\n",
      "    0    0]\n",
      "\n",
      "{'tokens': ['[CLS]', '▁미국', '이', '▁북', '핵', '▁위', '기', ',', '▁코', '소', '보', '▁전쟁', ',', '[MASK]', '[MASK]', '▁전쟁', '과', '▁같이', '▁미국', '이', '▁군사', '적', '▁행', '동을', '▁최', '후', '로', '▁선택', '하는', '▁전통', '적', '▁사고', '를', '▁버', '리고', '▁군사', '적', '▁행', '동을', '▁선', '행', '하는', '▁행', '위에', '▁나폴', '▁깊', '은', '▁유', '감을', '▁표시', '▁하며', '▁차례', '▁군사', '적', '▁활동', '에', '[MASK]', '▁반대', '▁입', '장을', '▁보', '이고', '▁있다', '.', '[SEP]', '▁특히', '▁국제', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁위해', '▁북한', '의', '▁김', '일', '성', ',', '▁아이', '티', '의', '▁세', '드', '라스', '▁장', '군', ',', '▁팔', '레', '인', '스타', '인의', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁보', '스', '니아', '의', '▁세르', '비아', '계', '▁정', '권', '▁같이', '▁미국', '▁정부', '에', '▁대해', '媛', '▁달성', '▁거부', '하면서', '▁사', '태', '의', '▁위', '기를', '▁초', '래', '한', '▁인물', '▁및', '▁단', '체를', '[MASK]', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [1, 2, 13, 14, 44, 51, 56, 67, 68, 69, 70, 92, 93, 94, 95, 110, 111, 126], 'mask_label': ['▁미국', '이', '▁이', '라크', '▁대해', '▁미국의', '▁강한', '▁분', '쟁', '▁조', '정을', '▁하', '마', '스', ',', '▁협', '상을', '▁직접']}\n",
      "enc_token: [5, 243, 3597, 251, 4166, 45, 3614, 3604, 258, 3688, 3672, 506, 3604, 6, 6, 506, 3644, 733, 243, 3597, 1250, 3657, 236, 1629, 130, 3706, 3603, 1715, 38, 1306, 3657, 1646, 3624, 407, 999, 1250, 3657, 236, 1629, 57, 3752, 38, 236, 1157, 3276, 1910, 3613, 46, 2196, 2466, 1368, 2037, 1250, 3657, 375, 3600, 6, 1216, 213, 480, 47, 458, 28, 3599, 4, 698, 605, 6, 6, 6, 6, 231, 1876, 3601, 200, 3629, 3650, 3604, 520, 3835, 3601, 74, 3681, 1951, 104, 3722, 3604, 961, 3740, 3628, 936, 692, 6, 6, 6, 6, 47, 3626, 491, 3601, 3189, 852, 3704, 36, 3830, 733, 243, 513, 3600, 433, 7583, 2666, 2324, 421, 15, 3800, 3601, 45, 333, 192, 3808, 3612, 1178, 228, 164, 1396, 6, 4]\n",
      "segment: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "label_nsp: 1\n",
      "label_mlm: [   0  243 3597    0    0    0    0    0    0    0    0    0    0    8\n",
      " 3553    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0  433    0    0    0    0    0    0  679    0    0    0    0\n",
      " 2632    0    0    0    0    0    0    0    0    0    0  147 3972   53\n",
      "  666    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0   27 3674 3626 3604    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0  617  460\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      " 1069    0]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_47/767648317.py:16: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  mask_idx = np.array(data[\"mask_idx\"], dtype=np.int)\n",
      "/tmp/ipykernel_47/767648317.py:17: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  mask_label = np.array([vocab.piece_to_id(p) for p in data[\"mask_label\"]], dtype=np.int)\n",
      "/tmp/ipykernel_47/767648317.py:18: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  label_mlm = np.full(n_seq, dtype=np.int, fill_value=0)\n"
     ]
    }
   ],
   "source": [
    "# 라인 단위로 처리\n",
    "with open(pretrain_json_path, \"r\") as f:\n",
    "    for i, line in enumerate(tqdm(f, total=total)):\n",
    "        if 5 < i:  # 테스트를 위해서 5개만 확인\n",
    "            break\n",
    "        data = json.loads(line)\n",
    "        # encoder token\n",
    "        enc_token = [vocab.piece_to_id(p) for p in data[\"tokens\"]]\n",
    "        enc_token += [0] * (n_seq - len(enc_token))\n",
    "        # segment\n",
    "        segment = data[\"segment\"]\n",
    "        segment += [0] * (n_seq - len(segment))\n",
    "        # nsp label\n",
    "        label_nsp = data[\"is_next\"]\n",
    "        # mlm label\n",
    "        mask_idx = np.array(data[\"mask_idx\"], dtype=np.int)\n",
    "        mask_label = np.array([vocab.piece_to_id(p) for p in data[\"mask_label\"]], dtype=np.int)\n",
    "        label_mlm = np.full(n_seq, dtype=np.int, fill_value=0)\n",
    "        label_mlm[mask_idx] = mask_label\n",
    "\n",
    "        print(data)\n",
    "        print(\"enc_token:\", enc_token)\n",
    "        print(\"segment:\", segment)\n",
    "        print(\"label_nsp:\", label_nsp)\n",
    "        print(\"label_mlm:\", label_mlm)\n",
    "        print()\n",
    "\n",
    "        assert len(enc_token) == len(segment) == len(label_mlm) == n_seq\n",
    "\n",
    "        enc_tokens[i] = enc_token\n",
    "        segments[i] = segment\n",
    "        labels_nsp[i] = label_nsp\n",
    "        labels_mlm[i] = label_mlm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1666192c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load_pre_train_data() 학습에 필요한 데이터를 로딩하는 함수\n",
    "# np.memap을 사용하여 메모리 효율적으로 만들어진 데이터를 로딩하는 함수\n",
    "\n",
    "def load_pre_train_data(vocab, filename, n_seq, count=None):\n",
    "    \"\"\"\n",
    "    학습에 필요한 데이터를 로드\n",
    "    :param vocab: vocab\n",
    "    :param filename: 전처리된 json 파일\n",
    "    :param n_seq: 시퀀스 길이 (number of sequence)\n",
    "    :param count: 데이터 수 제한 (None이면 전체)\n",
    "    :return enc_tokens: encoder inputs\n",
    "    :return segments: segment inputs\n",
    "    :return labels_nsp: nsp labels\n",
    "    :return labels_mlm: mlm labels\n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            total += 1\n",
    "            # 데이터 수 제한\n",
    "            if count is not None and count <= total:\n",
    "                break\n",
    "    \n",
    "    # np.memmap을 사용하면 메모리를 적은 메모리에서도 대용량 데이터 처리가 가능 함\n",
    "    enc_tokens = np.memmap(filename='enc_tokens.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "    segments = np.memmap(filename='segments.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "    labels_nsp = np.memmap(filename='labels_nsp.memmap', mode='w+', dtype=np.int32, shape=(total,))\n",
    "    labels_mlm = np.memmap(filename='labels_mlm.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "\n",
    "    with open(filename, \"r\") as f:\n",
    "        for i, line in enumerate(tqdm(f, total=total)):\n",
    "            if total <= i:\n",
    "                print(\"data load early stop\", total, i)\n",
    "                break\n",
    "            data = json.loads(line)\n",
    "            # encoder token\n",
    "            enc_token = [vocab.piece_to_id(p) for p in data[\"tokens\"]]\n",
    "            enc_token += [0] * (n_seq - len(enc_token))\n",
    "            # segment\n",
    "            segment = data[\"segment\"]\n",
    "            segment += [0] * (n_seq - len(segment))\n",
    "            # nsp label\n",
    "            label_nsp = data[\"is_next\"]\n",
    "            # mlm label\n",
    "            mask_idx = np.array(data[\"mask_idx\"], dtype=np.int)\n",
    "            mask_label = np.array([vocab.piece_to_id(p) for p in data[\"mask_label\"]], dtype=np.int)\n",
    "            label_mlm = np.full(n_seq, dtype=np.int, fill_value=0)\n",
    "            label_mlm[mask_idx] = mask_label\n",
    "\n",
    "            assert len(enc_token) == len(segment) == len(label_mlm) == n_seq\n",
    "\n",
    "            enc_tokens[i] = enc_token\n",
    "            segments[i] = segment\n",
    "            labels_nsp[i] = label_nsp\n",
    "            labels_mlm[i] = label_mlm\n",
    "\n",
    "    return (enc_tokens, segments), (labels_nsp, labels_mlm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "82a26e77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16953d69ee3d425d93430e68d233378b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_47/1644513872.py:45: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  mask_idx = np.array(data[\"mask_idx\"], dtype=np.int)\n",
      "/tmp/ipykernel_47/1644513872.py:46: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  mask_label = np.array([vocab.piece_to_id(p) for p in data[\"mask_label\"]], dtype=np.int)\n",
      "/tmp/ipykernel_47/1644513872.py:47: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  label_mlm = np.full(n_seq, dtype=np.int, fill_value=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data load early stop 200000 200000\n"
     ]
    }
   ],
   "source": [
    "# 128000건만 메모리에 로딩\n",
    "pre_train_inputs, pre_train_labels = load_pre_train_data(vocab, pretrain_json_path, 128, count=200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "754dd588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "#bert 모델 구현\n",
    "def get_pad_mask(tokens, i_pad=0):\n",
    "    \"\"\"\n",
    "    pad mask 계산하는 함수\n",
    "    :param tokens: tokens (bs, n_seq)\n",
    "    :param i_pad: id of pad\n",
    "    :return mask: pad mask (pad: 1, other: 0)\n",
    "    \"\"\"\n",
    "    mask = tf.cast(tf.math.equal(tokens, i_pad), tf.float32)\n",
    "    mask = tf.expand_dims(mask, axis=1)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def get_ahead_mask(tokens, i_pad=0):\n",
    "    \"\"\"\n",
    "    ahead mask 계산하는 함수\n",
    "    :param tokens: tokens (bs, n_seq)\n",
    "    :param i_pad: id of pad\n",
    "    :return mask: ahead and pad mask (ahead or pad: 1, other: 0)\n",
    "    \"\"\"\n",
    "    n_seq = tf.shape(tokens)[1]\n",
    "    ahead_mask = 1 - tf.linalg.band_part(tf.ones((n_seq, n_seq)), -1, 0)\n",
    "    ahead_mask = tf.expand_dims(ahead_mask, axis=0)\n",
    "    pad_mask = get_pad_mask(tokens, i_pad)\n",
    "    mask = tf.maximum(ahead_mask, pad_mask)\n",
    "    return mask\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "eec80a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "@tf.function(experimental_relax_shapes=True)\n",
    "def gelu(x):\n",
    "    \"\"\"\n",
    "    gelu activation 함수\n",
    "    :param x: 입력 값\n",
    "    :return: gelu activation result\n",
    "    \"\"\"\n",
    "    return 0.5*x*(1+tf.tanh(np.sqrt(2/np.pi)*(x+0.044715*tf.pow(x, 3))))\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "82f6e3e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def kernel_initializer(stddev=0.02):\n",
    "    \"\"\"\n",
    "    parameter initializer 생성\n",
    "    :param stddev: 생성할 랜덤 변수의 표준편차\n",
    "    \"\"\"\n",
    "    return tf.keras.initializers.TruncatedNormal(stddev=stddev)\n",
    "\n",
    "\n",
    "def bias_initializer():\n",
    "    \"\"\"\n",
    "    bias initializer 생성\n",
    "    \"\"\"\n",
    "    return tf.zeros_initializer\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1d61bfd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class Config(dict):\n",
    "    \"\"\"\n",
    "    json을 config 형태로 사용하기 위한 Class\n",
    "    :param dict: config dictionary\n",
    "    \"\"\"\n",
    "    __getattr__ = dict.__getitem__\n",
    "    __setattr__ = dict.__setitem__\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, file):\n",
    "        \"\"\"\n",
    "        file에서 Config를 생성 함\n",
    "        :param file: filename\n",
    "        \"\"\"\n",
    "        with open(file, 'r') as f:\n",
    "            config = json.loads(f.read())\n",
    "            return Config(config)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a885cdda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class SharedEmbedding(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Weighed Shaed Embedding Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"weight_shared_embedding\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.n_vocab = config.n_vocab\n",
    "        self.d_model = config.d_model\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        \"\"\"\n",
    "        shared weight 생성\n",
    "        :param input_shape: Tensor Shape (not used)\n",
    "        \"\"\"\n",
    "        with tf.name_scope(\"shared_embedding_weight\"):\n",
    "            self.shared_weights = self.add_weight(\n",
    "                \"weights\",\n",
    "                shape=[self.n_vocab, self.d_model],\n",
    "                initializer=kernel_initializer()\n",
    "            )\n",
    "\n",
    "    def call(self, inputs, mode=\"embedding\"):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: 입력\n",
    "        :param mode: 실행 모드\n",
    "        :return: embedding or linear 실행 결과\n",
    "        \"\"\"\n",
    "        # mode가 embedding일 경우 embedding lookup 실행\n",
    "        if mode == \"embedding\":\n",
    "            return self._embedding(inputs)\n",
    "        # mode가 linear일 경우 linear 실행\n",
    "        elif mode == \"linear\":\n",
    "            return self._linear(inputs)\n",
    "        # mode가 기타일 경우 오류 발생\n",
    "        else:\n",
    "            raise ValueError(f\"mode {mode} is not valid.\")\n",
    "    \n",
    "    def _embedding(self, inputs):\n",
    "        \"\"\"\n",
    "        embedding lookup\n",
    "        :param inputs: 입력\n",
    "        \"\"\"\n",
    "        embed = tf.gather(self.shared_weights, tf.cast(inputs, tf.int32))\n",
    "        return embed\n",
    "\n",
    "    def _linear(self, inputs):  # (bs, n_seq, d_model)\n",
    "        \"\"\"\n",
    "        linear 실행\n",
    "        :param inputs: 입력\n",
    "        \"\"\"\n",
    "        n_batch = tf.shape(inputs)[0]\n",
    "        n_seq = tf.shape(inputs)[1]\n",
    "        inputs = tf.reshape(inputs, [-1, self.d_model])  # (bs * n_seq, d_model)\n",
    "        outputs = tf.matmul(inputs, self.shared_weights, transpose_b=True)\n",
    "        outputs = tf.reshape(outputs, [n_batch, n_seq, self.n_vocab])  # (bs, n_seq, n_vocab)\n",
    "        return outputs\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7888d915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class PositionEmbedding(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Position Embedding Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"position_embedding\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(config.n_seq, config.d_model, embeddings_initializer=kernel_initializer())\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: 입력\n",
    "        :return embed: position embedding lookup 결과\n",
    "        \"\"\"\n",
    "        position = tf.cast(tf.math.cumsum(tf.ones_like(inputs), axis=1, exclusive=True), tf.int32)\n",
    "        embed = self.embedding(position)\n",
    "        return embed\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "47c17fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class ScaleDotProductAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Scale Dot Product Attention Class\n",
    "    \"\"\"\n",
    "    def __init__(self, name=\"scale_dot_product_attention\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "    def call(self, Q, K, V, attn_mask):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param Q: Q value\n",
    "        :param K: K value\n",
    "        :param V: V value\n",
    "        :param attn_mask: 실행 모드\n",
    "        :return attn_out: attention 실행 결과\n",
    "        \"\"\"\n",
    "        attn_score = tf.matmul(Q, K, transpose_b=True)\n",
    "        scale = tf.math.sqrt(tf.cast(tf.shape(K)[-1], tf.float32))\n",
    "        attn_scale = tf.math.divide(attn_score, scale)\n",
    "        attn_scale -= 1.e9 * attn_mask\n",
    "        attn_prob = tf.nn.softmax(attn_scale, axis=-1)\n",
    "        attn_out = tf.matmul(attn_prob, V)\n",
    "        return attn_out\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c925120d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Multi Head Attention Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"multi_head_attention\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.d_model = config.d_model\n",
    "        self.n_head = config.n_head\n",
    "        self.d_head = config.d_head\n",
    "\n",
    "        # Q, K, V input dense layer\n",
    "        self.W_Q = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.W_K = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.W_V = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        # Scale Dot Product Attention class\n",
    "        self.attention = ScaleDotProductAttention(name=\"self_attention\")\n",
    "        # output dense layer\n",
    "        self.W_O = tf.keras.layers.Dense(config.d_model, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "\n",
    "    def call(self, Q, K, V, attn_mask):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param Q: Q value\n",
    "        :param K: K value\n",
    "        :param V: V value\n",
    "        :param attn_mask: 실행 모드\n",
    "        :return attn_out: attention 실행 결과\n",
    "        \"\"\"\n",
    "        # reshape Q, K, V, attn_mask\n",
    "        batch_size = tf.shape(Q)[0]\n",
    "        Q_m = tf.transpose(tf.reshape(self.W_Q(Q), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, Q_len, d_head)\n",
    "        K_m = tf.transpose(tf.reshape(self.W_K(K), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, K_len, d_head)\n",
    "        V_m = tf.transpose(tf.reshape(self.W_V(V), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, K_len, d_head)\n",
    "        attn_mask_m = tf.expand_dims(attn_mask, axis=1)\n",
    "        # Scale Dot Product Attention with multi head Q, K, V, attn_mask\n",
    "        attn_out = self.attention(Q_m, K_m, V_m, attn_mask_m)  # (bs, n_head, Q_len, d_head)\n",
    "        # transpose and liner\n",
    "        attn_out_m = tf.transpose(attn_out, perm=[0, 2, 1, 3])  # (bs, Q_len, n_head, d_head)\n",
    "        attn_out = tf.reshape(attn_out_m, [batch_size, -1, config.n_head * config.d_head])  # (bs, Q_len, d_model)\n",
    "        attn_out = self.W_O(attn_out) # (bs, Q_len, d_model)\n",
    "\n",
    "        return attn_out\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "281b130d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class PositionWiseFeedForward(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Position Wise Feed Forward Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"feed_forward\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.W_1 = tf.keras.layers.Dense(config.d_ff, activation=gelu, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.W_2 = tf.keras.layers.Dense(config.d_model, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: inputs\n",
    "        :return ff_val: feed forward 실행 결과\n",
    "        \"\"\"\n",
    "        ff_val = self.W_2(self.W_1(inputs))\n",
    "        return ff_val\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "94153267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Encoder Layer Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"encoder_layer\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.self_attention = MultiHeadAttention(config)\n",
    "        self.norm1 = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "\n",
    "        self.ffn = PositionWiseFeedForward(config)\n",
    "        self.norm2 = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(config.dropout)\n",
    " \n",
    "    def call(self, enc_embed, self_mask):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param enc_embed: enc_embed 또는 이전 EncoderLayer의 출력\n",
    "        :param self_mask: enc_tokens의 pad mask\n",
    "        :return enc_out: EncoderLayer 실행 결과\n",
    "        \"\"\"\n",
    "        self_attn_val = self.self_attention(enc_embed, enc_embed, enc_embed, self_mask)\n",
    "        norm1_val = self.norm1(enc_embed + self.dropout(self_attn_val))\n",
    "\n",
    "        ffn_val = self.ffn(norm1_val)\n",
    "        enc_out = self.norm2(norm1_val + self.dropout(ffn_val))\n",
    "\n",
    "        return enc_out\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "757bea8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class BERT(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    BERT Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"bert\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.i_pad = config.i_pad\n",
    "        self.embedding = SharedEmbedding(config)\n",
    "        self.position = PositionEmbedding(config)\n",
    "        self.segment = tf.keras.layers.Embedding(2, config.d_model, embeddings_initializer=kernel_initializer())\n",
    "        self.norm = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "        \n",
    "        self.encoder_layers = [EncoderLayer(config, name=f\"encoder_layer_{i}\") for i in range(config.n_layer)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(config.dropout)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: (enc_tokens, segments)\n",
    "        :return logits: dec_tokens에 대한 다음 토큰 예측 결과 logits\n",
    "        \"\"\"\n",
    "        enc_tokens, segments = inputs\n",
    "\n",
    "        enc_self_mask = tf.keras.layers.Lambda(get_pad_mask, output_shape=(1, None), name='enc_self_mask')(enc_tokens, self.i_pad)\n",
    "\n",
    "        enc_embed = self.get_embedding(enc_tokens, segments)\n",
    "\n",
    "        enc_out = self.dropout(enc_embed)\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            enc_out = encoder_layer(enc_out, enc_self_mask)\n",
    "\n",
    "        logits_cls = enc_out[:,0]\n",
    "        logits_lm = self.embedding(enc_out, mode=\"linear\")\n",
    "        return logits_cls, logits_lm\n",
    "    \n",
    "    def get_embedding(self, tokens, segments):\n",
    "        \"\"\"\n",
    "        token embedding, position embedding lookup\n",
    "        :param tokens: 입력 tokens\n",
    "        :param segments: 입력 segments\n",
    "        :return embed: embedding 결과\n",
    "        \"\"\"\n",
    "        embed = self.embedding(tokens) + self.position(tokens) + self.segment(segments)\n",
    "        embed = self.norm(embed)\n",
    "        return embed\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d9bc7e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# Encoder Layer class 정의\n",
    "class PooledOutput(tf.keras.layers.Layer):\n",
    "    def __init__(self, config, n_output, name=\"pooled_output\"):\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.dense1 = tf.keras.layers.Dense(config.d_model, activation=tf.nn.tanh, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.dense2 = tf.keras.layers.Dense(n_output, use_bias=False, activation=tf.nn.softmax, name=\"nsp\", kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    " \n",
    "    def call(self, inputs):\n",
    "        outputs = self.dense1(inputs)\n",
    "        outputs = self.dense2(outputs)\n",
    "        return outputs\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4b76ba44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def build_model_pre_train(config):\n",
    "    enc_tokens = tf.keras.layers.Input((None,), name=\"enc_tokens\")\n",
    "    segments = tf.keras.layers.Input((None,), name=\"segments\")\n",
    "\n",
    "    bert = BERT(config)\n",
    "    logits_cls, logits_lm = bert((enc_tokens, segments))\n",
    "\n",
    "    logits_cls = PooledOutput(config, 2, name=\"pooled_nsp\")(logits_cls)\n",
    "    outputs_nsp = tf.keras.layers.Softmax(name=\"nsp\")(logits_cls)\n",
    "\n",
    "    outputs_mlm = tf.keras.layers.Softmax(name=\"mlm\")(logits_lm)\n",
    "\n",
    "    model = tf.keras.Model(inputs=(enc_tokens, segments), outputs=(outputs_nsp, outputs_mlm))\n",
    "    return model\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7861f5ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'d_model': 256,\n",
       " 'n_head': 4,\n",
       " 'd_head': 64,\n",
       " 'dropout': 0.1,\n",
       " 'd_ff': 1024,\n",
       " 'layernorm_epsilon': 0.001,\n",
       " 'n_layer': 3,\n",
       " 'n_seq': 256,\n",
       " 'n_vocab': 8007,\n",
       " 'i_pad': 0}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = Config({\"d_model\": 256, \"n_head\": 4, \"d_head\": 64, \"dropout\": 0.1, \"d_ff\": 1024, \"layernorm_epsilon\": 0.001, \"n_layer\": 3, \"n_seq\": 256, \"n_vocab\": 0, \"i_pad\": 0})\n",
    "config.n_vocab = len(vocab)\n",
    "config.i_pad = vocab.pad_id()\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f7a231b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def lm_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    loss 계산 함수\n",
    "    :param y_true: 정답 (bs, n_seq)\n",
    "    :param y_pred: 예측 값 (bs, n_seq, n_vocab)\n",
    "    \"\"\"\n",
    "    # loss 계산\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE)(y_true, y_pred)\n",
    "    # pad(0) 인 부분 mask\n",
    "    mask = tf.cast(tf.math.not_equal(y_true, 0), dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "    return loss * 20  # mlm을 더 잘 학습하도록 20배 증가 시킴\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "79114689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def lm_acc(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    acc 계산 함수\n",
    "    :param y_true: 정답 (bs, n_seq)\n",
    "    :param y_pred: 예측 값 (bs, n_seq, n_vocab)\n",
    "    \"\"\"\n",
    "    # 정답 여부 확인\n",
    "    y_pred_class = tf.cast(K.argmax(y_pred, axis=-1), tf.float32)\n",
    "    matches = tf.cast(K.equal(y_true, y_pred_class), tf.float32)\n",
    "    # pad(0) 인 부분 mask\n",
    "    mask = tf.cast(tf.math.not_equal(y_true, 0), dtype=matches.dtype)\n",
    "    matches *= mask\n",
    "    # 정확도 계산\n",
    "    accuracy = K.sum(matches) / K.maximum(K.sum(mask), 1)\n",
    "    return accuracy\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "17447fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class CosineSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \"\"\"\n",
    "    CosineSchedule Class\n",
    "    \"\"\"\n",
    "    def __init__(self, train_steps=4000, warmup_steps=2000, max_lr=2.5e-4):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param train_steps: 학습 step 총 합\n",
    "        :param warmup_steps: warmup steps\n",
    "        :param max_lr: 최대 learning rate\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        assert 0 < warmup_steps < train_steps\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.train_steps = train_steps\n",
    "        self.max_lr = max_lr\n",
    "\n",
    "    def __call__(self, step_num):\n",
    "        \"\"\"\n",
    "        learning rate 계산\n",
    "        :param step_num: 현재 step number\n",
    "        :retrun: 계산된 learning rate\n",
    "        \"\"\"\n",
    "        state = tf.cast(step_num <= self.warmup_steps, tf.float32)\n",
    "        lr1 = tf.cast(step_num, tf.float32) / self.warmup_steps\n",
    "        progress = tf.cast(step_num - self.warmup_steps, tf.float32) / max(1, self.train_steps - self.warmup_steps)\n",
    "        lr2 = 0.5 * (1.0 + tf.math.cos(math.pi * progress))\n",
    "        return (state * lr1 + (1 - state) * lr2) * self.max_lr\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "31e5ea96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# compute lr \n",
    "test_schedule = CosineSchedule(train_steps=4000, warmup_steps=500)\n",
    "lrs = []\n",
    "for step_num in range(4000):\n",
    "    lrs.append(test_schedule(float(step_num)).numpy())\n",
    "\n",
    "# draw\n",
    "plt.plot(lrs, 'r-', label='learning_rate')\n",
    "plt.xlabel('Step')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "0f7f78d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "enc_tokens (InputLayer)         [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segments (InputLayer)           [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert (BERT)                     ((None, 256), (None, 4485632     enc_tokens[0][0]                 \n",
      "                                                                 segments[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pooled_nsp (PooledOutput)       (None, 2)            66304       bert[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "nsp (Softmax)                   (None, 2)            0           pooled_nsp[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "mlm (Softmax)                   (None, None, 8007)   0           bert[0][1]                       \n",
      "==================================================================================================\n",
      "Total params: 4,551,936\n",
      "Trainable params: 4,551,936\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 모델 생성\n",
    "pre_train_model = build_model_pre_train(config)\n",
    "pre_train_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "559475e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_steps: 62500\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "batch_size = 64\n",
    "\n",
    "# optimizer\n",
    "train_steps = math.ceil(len(pre_train_inputs[0]) / batch_size) * epochs\n",
    "print(\"train_steps:\", train_steps)\n",
    "learning_rate = CosineSchedule(train_steps=train_steps, warmup_steps=max(100, train_steps // 10))\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "# compile\n",
    "pre_train_model.compile(loss=(tf.keras.losses.sparse_categorical_crossentropy, lm_loss), optimizer=optimizer, metrics={\"nsp\": \"acc\", \"mlm\": lm_acc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e084d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 500000의 데이터 1회 train하는데 17분걸리는데 흠.. \n",
    "# 데이터 20만으로 줄이고 epoch를 늘리는 방법으로 진행.\n",
    "# 아 같은거 다시 돌렸네...\n",
    "\n",
    "# # save weights callback\n",
    "# save_weights = tf.keras.callbacks.ModelCheckpoint(f\"{model_dir}/bert_pre_train.hdf5\", monitor=\"mlm_lm_acc\", verbose=1, save_best_only=True, mode=\"max\", save_freq=\"epoch\", save_weights_only=True)\n",
    "# # train\n",
    "# history = pre_train_model.fit(pre_train_inputs, pre_train_labels, epochs=epochs, batch_size=batch_size, callbacks=[save_weights])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "6f4c89b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "3125/3125 [==============================] - 392s 125ms/step - loss: 19.5929 - nsp_loss: 0.6438 - mlm_loss: 18.9491 - nsp_acc: 0.6019 - mlm_lm_acc: 0.1066\n",
      "\n",
      "Epoch 00001: mlm_lm_acc improved from -inf to 0.10665, saving model to /aiffel/aiffel/prj/bert_pre_train.hdf5\n",
      "Epoch 2/20\n",
      "3125/3125 [==============================] - 389s 124ms/step - loss: 17.4600 - nsp_loss: 0.6119 - mlm_loss: 16.8481 - nsp_acc: 0.6344 - mlm_lm_acc: 0.1292\n",
      "\n",
      "Epoch 00002: mlm_lm_acc improved from 0.10665 to 0.12917, saving model to /aiffel/aiffel/prj/bert_pre_train.hdf5\n",
      "Epoch 3/20\n",
      "3125/3125 [==============================] - 389s 124ms/step - loss: 14.9447 - nsp_loss: 0.6034 - mlm_loss: 14.3413 - nsp_acc: 0.6419 - mlm_lm_acc: 0.1666\n",
      "\n",
      "Epoch 00003: mlm_lm_acc improved from 0.12917 to 0.16657, saving model to /aiffel/aiffel/prj/bert_pre_train.hdf5\n",
      "Epoch 4/20\n",
      "3125/3125 [==============================] - 389s 125ms/step - loss: 13.3978 - nsp_loss: 0.5982 - mlm_loss: 12.7996 - nsp_acc: 0.6467 - mlm_lm_acc: 0.2096\n",
      "\n",
      "Epoch 00004: mlm_lm_acc improved from 0.16657 to 0.20963, saving model to /aiffel/aiffel/prj/bert_pre_train.hdf5\n",
      "Epoch 5/20\n",
      "3125/3125 [==============================] - 389s 124ms/step - loss: 12.7561 - nsp_loss: 0.5945 - mlm_loss: 12.1616 - nsp_acc: 0.6527 - mlm_lm_acc: 0.2301\n",
      "\n",
      "Epoch 00005: mlm_lm_acc improved from 0.20963 to 0.23008, saving model to /aiffel/aiffel/prj/bert_pre_train.hdf5\n",
      "Epoch 6/20\n",
      "3125/3125 [==============================] - 389s 125ms/step - loss: 12.3418 - nsp_loss: 0.5921 - mlm_loss: 11.7496 - nsp_acc: 0.6580 - mlm_lm_acc: 0.2450\n",
      "\n",
      "Epoch 00006: mlm_lm_acc improved from 0.23008 to 0.24503, saving model to /aiffel/aiffel/prj/bert_pre_train.hdf5\n",
      "Epoch 7/20\n",
      "3125/3125 [==============================] - 389s 124ms/step - loss: 12.0339 - nsp_loss: 0.5896 - mlm_loss: 11.4443 - nsp_acc: 0.6631 - mlm_lm_acc: 0.2564\n",
      "\n",
      "Epoch 00007: mlm_lm_acc improved from 0.24503 to 0.25637, saving model to /aiffel/aiffel/prj/bert_pre_train.hdf5\n",
      "Epoch 8/20\n",
      "3125/3125 [==============================] - 389s 125ms/step - loss: 11.7915 - nsp_loss: 0.5872 - mlm_loss: 11.2043 - nsp_acc: 0.6683 - mlm_lm_acc: 0.2651\n",
      "\n",
      "Epoch 00008: mlm_lm_acc improved from 0.25637 to 0.26509, saving model to /aiffel/aiffel/prj/bert_pre_train.hdf5\n",
      "Epoch 9/20\n",
      "3125/3125 [==============================] - 389s 125ms/step - loss: 11.5892 - nsp_loss: 0.5846 - mlm_loss: 11.0046 - nsp_acc: 0.6738 - mlm_lm_acc: 0.2727\n",
      "\n",
      "Epoch 00009: mlm_lm_acc improved from 0.26509 to 0.27269, saving model to /aiffel/aiffel/prj/bert_pre_train.hdf5\n",
      "Epoch 10/20\n",
      "3125/3125 [==============================] - 389s 125ms/step - loss: 11.4166 - nsp_loss: 0.5815 - mlm_loss: 10.8351 - nsp_acc: 0.6803 - mlm_lm_acc: 0.2788\n",
      "\n",
      "Epoch 00010: mlm_lm_acc improved from 0.27269 to 0.27882, saving model to /aiffel/aiffel/prj/bert_pre_train.hdf5\n",
      "Epoch 11/20\n",
      "3125/3125 [==============================] - 389s 124ms/step - loss: 11.2656 - nsp_loss: 0.5782 - mlm_loss: 10.6874 - nsp_acc: 0.6860 - mlm_lm_acc: 0.2844\n",
      "\n",
      "Epoch 00011: mlm_lm_acc improved from 0.27882 to 0.28440, saving model to /aiffel/aiffel/prj/bert_pre_train.hdf5\n",
      "Epoch 12/20\n",
      "3125/3125 [==============================] - 389s 124ms/step - loss: 11.1336 - nsp_loss: 0.5748 - mlm_loss: 10.5588 - nsp_acc: 0.6933 - mlm_lm_acc: 0.2894\n",
      "\n",
      "Epoch 00012: mlm_lm_acc improved from 0.28440 to 0.28941, saving model to /aiffel/aiffel/prj/bert_pre_train.hdf5\n",
      "Epoch 13/20\n",
      "3125/3125 [==============================] - 389s 124ms/step - loss: 11.0128 - nsp_loss: 0.5703 - mlm_loss: 10.4425 - nsp_acc: 0.7005 - mlm_lm_acc: 0.2938\n",
      "\n",
      "Epoch 00013: mlm_lm_acc improved from 0.28941 to 0.29376, saving model to /aiffel/aiffel/prj/bert_pre_train.hdf5\n",
      "Epoch 14/20\n",
      "3125/3125 [==============================] - 389s 124ms/step - loss: 10.9083 - nsp_loss: 0.5659 - mlm_loss: 10.3424 - nsp_acc: 0.7076 - mlm_lm_acc: 0.2977\n",
      "\n",
      "Epoch 00014: mlm_lm_acc improved from 0.29376 to 0.29768, saving model to /aiffel/aiffel/prj/bert_pre_train.hdf5\n",
      "Epoch 15/20\n",
      "3125/3125 [==============================] - 389s 124ms/step - loss: 10.8173 - nsp_loss: 0.5618 - mlm_loss: 10.2555 - nsp_acc: 0.7154 - mlm_lm_acc: 0.3011\n",
      "\n",
      "Epoch 00015: mlm_lm_acc improved from 0.29768 to 0.30114, saving model to /aiffel/aiffel/prj/bert_pre_train.hdf5\n",
      "Epoch 16/20\n",
      "3125/3125 [==============================] - 389s 124ms/step - loss: 10.7410 - nsp_loss: 0.5578 - mlm_loss: 10.1831 - nsp_acc: 0.7215 - mlm_lm_acc: 0.3041\n",
      "\n",
      "Epoch 00016: mlm_lm_acc improved from 0.30114 to 0.30408, saving model to /aiffel/aiffel/prj/bert_pre_train.hdf5\n",
      "Epoch 17/20\n",
      "3125/3125 [==============================] - 389s 124ms/step - loss: 10.6797 - nsp_loss: 0.5541 - mlm_loss: 10.1256 - nsp_acc: 0.7267 - mlm_lm_acc: 0.3064\n",
      "\n",
      "Epoch 00017: mlm_lm_acc improved from 0.30408 to 0.30637, saving model to /aiffel/aiffel/prj/bert_pre_train.hdf5\n",
      "Epoch 18/20\n",
      "3125/3125 [==============================] - 389s 124ms/step - loss: 10.6330 - nsp_loss: 0.5515 - mlm_loss: 10.0815 - nsp_acc: 0.7316 - mlm_lm_acc: 0.3080\n",
      "\n",
      "Epoch 00018: mlm_lm_acc improved from 0.30637 to 0.30800, saving model to /aiffel/aiffel/prj/bert_pre_train.hdf5\n",
      "Epoch 19/20\n",
      "3125/3125 [==============================] - 389s 125ms/step - loss: 10.6038 - nsp_loss: 0.5495 - mlm_loss: 10.0543 - nsp_acc: 0.7345 - mlm_lm_acc: 0.3091\n",
      "\n",
      "Epoch 00019: mlm_lm_acc improved from 0.30800 to 0.30911, saving model to /aiffel/aiffel/prj/bert_pre_train.hdf5\n",
      "Epoch 20/20\n",
      "3125/3125 [==============================] - 389s 124ms/step - loss: 10.5890 - nsp_loss: 0.5486 - mlm_loss: 10.0404 - nsp_acc: 0.7360 - mlm_lm_acc: 0.3096\n",
      "\n",
      "Epoch 00020: mlm_lm_acc improved from 0.30911 to 0.30956, saving model to /aiffel/aiffel/prj/bert_pre_train.hdf5\n"
     ]
    }
   ],
   "source": [
    "# save weights callback\n",
    "save_weights = tf.keras.callbacks.ModelCheckpoint(f\"{model_dir}/bert_pre_train.hdf5\", monitor=\"mlm_lm_acc\", verbose=1, save_best_only=True, mode=\"max\", save_freq=\"epoch\", save_weights_only=True)\n",
    "# train\n",
    "history = pre_train_model.fit(pre_train_inputs, pre_train_labels, epochs=epochs, batch_size=batch_size, callbacks=[save_weights])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ef83f4a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# training result\n",
    "plt.figure(figsize=(16, 4))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "# plt.plot(history.history['nsp_loss'], 'b-', label='nsp_loss')\n",
    "plt.plot(history.history['mlm_loss'], 'r--', label='mlm_loss')\n",
    "plt.title('mlm_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(history.history['nsp_acc'], 'g-', label='nsp_acc')\n",
    "plt.plot(history.history['mlm_lm_acc'], 'k--', label='mlm_acc')\n",
    "plt.xlabel('Epoch')\n",
    "plt.title('nsp&mlm acc')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(history.history['nsp_loss'], 'b-', label='nsp_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.title('nsp_loss')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199c627c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
