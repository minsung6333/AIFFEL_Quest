{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f6355b2",
   "metadata": {},
   "source": [
    "# 14. Transformer 번역기 만들기\n",
    "\n",
    "## 1. Transformer모델 생성\n",
    "### 1) positional_encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47d3111d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    " \n",
    "fontpath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n",
    "font = fm.FontProperties(fname=fontpath, size=9)\n",
    "plt.rc('font', family='NanumBarunGothic') \n",
    "mpl.font_manager.findfont(font)\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "370356a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "import random\n",
    "\n",
    "import seaborn # Attention 시각화를 위해 필요!\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ed827a",
   "metadata": {},
   "source": [
    "### 1) positional_encoding 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec6e251b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def positional_encoding(pos, d_model):\n",
    "    def cal_angle(position, i):\n",
    "        return position / np.power(10000, int(i) / d_model)\n",
    "\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i) for i in range(d_model)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
    "    return sinusoid_table\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d1d7e7",
   "metadata": {},
   "source": [
    "### 2) MultiHeadAttention 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b35ce20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "            \n",
    "        self.depth = d_model // self.num_heads\n",
    "            \n",
    "        self.W_q = tf.keras.layers.Dense(d_model)\n",
    "        self.W_k = tf.keras.layers.Dense(d_model)\n",
    "        self.W_v = tf.keras.layers.Dense(d_model)\n",
    "            \n",
    "        self.linear = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
    "        d_k = tf.cast(K.shape[-1], tf.float32)\n",
    "        QK = tf.matmul(Q, K, transpose_b=True)\n",
    "\n",
    "        scaled_qk = QK / tf.math.sqrt(d_k)\n",
    "\n",
    "        if mask is not None: scaled_qk += (mask * -1e9)  \n",
    "\n",
    "        attentions = tf.nn.softmax(scaled_qk, axis=-1)\n",
    "        out = tf.matmul(attentions, V)\n",
    "\n",
    "        return out, attentions\n",
    "            \n",
    "\n",
    "    def split_heads(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        split_x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        split_x = tf.transpose(split_x, perm=[0, 2, 1, 3])\n",
    "\n",
    "        return split_x\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        combined_x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        combined_x = tf.reshape(combined_x, (batch_size, -1, self.d_model))\n",
    "\n",
    "        return combined_x\n",
    "\n",
    "        \n",
    "    def call(self, Q, K, V, mask):\n",
    "        WQ = self.W_q(Q)\n",
    "        WK = self.W_k(K)\n",
    "        WV = self.W_v(V)\n",
    "        \n",
    "        WQ_splits = self.split_heads(WQ)\n",
    "        WK_splits = self.split_heads(WK)\n",
    "        WV_splits = self.split_heads(WV)\n",
    "            \n",
    "        out, attention_weights = self.scaled_dot_product_attention(\n",
    "            WQ_splits, WK_splits, WV_splits, mask)\n",
    "     \n",
    "        out = self.combine_heads(out)\n",
    "        out = self.linear(out)\n",
    "                \n",
    "        return out, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b070823",
   "metadata": {},
   "source": [
    "### 3) poswiseFeedforward network 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75b147cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.w_1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
    "        self.w_2 = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.w_1(x)\n",
    "        out = self.w_2(out)\n",
    "            \n",
    "        return out\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401c7290",
   "metadata": {},
   "source": [
    "### 4) encoder layer 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16e394ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "\n",
    "        \"\"\"\n",
    "        Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "        \n",
    "        \"\"\"\n",
    "        Position-Wise Feed Forward Network\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "        \n",
    "        return out, enc_attn\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddb45bd",
   "metadata": {},
   "source": [
    "### 5) decoder layer 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dae20615",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "    \n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "\n",
    "        \"\"\"\n",
    "        Masked Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, dec_attn = self.dec_self_attn(out, out, out, padding_mask)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "\n",
    "        \"\"\"\n",
    "        Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out, dec_enc_attn = self.enc_dec_attn(out, enc_out, enc_out, causality_mask)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "        \n",
    "        \"\"\"\n",
    "        Position-Wise Feed Forward Network\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_3(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "\n",
    "        return out, dec_attn, dec_enc_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc930b57",
   "metadata": {},
   "source": [
    "### 6) Transformer encoder 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a55e3ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                 n_layers,\n",
    "                 d_model,\n",
    "                 n_heads,\n",
    "                 d_ff,\n",
    "                 dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                        for _ in range(n_layers)]\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        out = x\n",
    "    \n",
    "        enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, enc_attn = self.enc_layers[i](out, mask)\n",
    "            enc_attns.append(enc_attn)\n",
    "        \n",
    "        return out, enc_attns\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47f9402",
   "metadata": {},
   "source": [
    "### 7) Transformer decoder 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34389105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                 n_layers,\n",
    "                 d_model,\n",
    "                 n_heads,\n",
    "                 d_ff,\n",
    "                 dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                            for _ in range(n_layers)]\n",
    "                            \n",
    "                            \n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "        out = x\n",
    "    \n",
    "        dec_attns = list()\n",
    "        dec_enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, dec_attn, dec_enc_attn = \\\n",
    "            self.dec_layers[i](out, enc_out, causality_mask, padding_mask)\n",
    "\n",
    "            dec_attns.append(dec_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "\n",
    "        return out, dec_attns, dec_enc_attns\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e5ab0b",
   "metadata": {},
   "source": [
    "### 8) Transformer 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "991cf421",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    src_vocab_size,\n",
    "                    tgt_vocab_size,\n",
    "                    pos_len,\n",
    "                    dropout=0.2,\n",
    "                    shared=True):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "\n",
    "        self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "        self.dec_emb = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
    "\n",
    "        self.pos_encoding = positional_encoding(pos_len, d_model)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "\n",
    "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n",
    "\n",
    "        self.shared = shared\n",
    "\n",
    "        if shared: self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n",
    "\n",
    "    def embedding(self, emb, x):\n",
    "        seq_len = x.shape[1]\n",
    "        out = emb(x)\n",
    "\n",
    "        if self.shared: out *= tf.math.sqrt(self.d_model)\n",
    "\n",
    "        out += self.pos_encoding[np.newaxis, ...][:, :seq_len, :]\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "        \n",
    "    def call(self, enc_in, dec_in, enc_mask, causality_mask, dec_mask):\n",
    "        enc_in = self.embedding(self.enc_emb, enc_in)\n",
    "        dec_in = self.embedding(self.dec_emb, dec_in)\n",
    "\n",
    "        enc_out, enc_attns = self.encoder(enc_in, enc_mask)\n",
    "        \n",
    "        dec_out, dec_attns, dec_enc_attns = \\\n",
    "        self.decoder(dec_in, enc_out, causality_mask, dec_mask)\n",
    "        \n",
    "        logits = self.fc(dec_out)\n",
    "        \n",
    "        return logits, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffe4eba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def generate_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def generate_causality_mask(src_len, tgt_len):\n",
    "    mask = 1 - np.cumsum(np.eye(src_len, tgt_len), 0)\n",
    "    return tf.cast(mask, tf.float32)\n",
    "\n",
    "def generate_masks(src, tgt):\n",
    "    enc_mask = generate_padding_mask(src)\n",
    "    dec_mask = generate_padding_mask(tgt)\n",
    "\n",
    "    dec_enc_causality_mask = generate_causality_mask(tgt.shape[1], src.shape[1])\n",
    "    dec_enc_mask = tf.maximum(enc_mask, dec_enc_causality_mask)\n",
    "\n",
    "    dec_causality_mask = generate_causality_mask(tgt.shape[1], tgt.shape[1])\n",
    "    dec_mask = tf.maximum(dec_mask, dec_causality_mask)\n",
    "\n",
    "    return enc_mask, dec_enc_mask, dec_mask\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "350e87d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 3 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 208,
       "width": 426
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "batch, length = 16, 20\n",
    "src_padding = 5\n",
    "tgt_padding = 15\n",
    "\n",
    "src_pad = tf.zeros(shape=(batch, src_padding))\n",
    "tgt_pad = tf.zeros(shape=(batch, tgt_padding))\n",
    "\n",
    "sample_data = tf.ones(shape=(batch, length))\n",
    "\n",
    "sample_src = tf.concat([sample_data, src_pad], axis=-1)\n",
    "sample_tgt = tf.concat([sample_data, tgt_pad], axis=-1)\n",
    "\n",
    "enc_mask, dec_enc_mask, dec_mask = \\\n",
    "generate_masks(sample_src, sample_tgt)\n",
    "\n",
    "fig = plt.figure(figsize=(7, 7))\n",
    "\n",
    "ax1 = fig.add_subplot(131)\n",
    "ax2 = fig.add_subplot(132)\n",
    "ax3 = fig.add_subplot(133)\n",
    "\n",
    "ax1.set_title('1) Encoder Mask')\n",
    "ax2.set_title('2) Encoder-Decoder Mask')\n",
    "ax3.set_title('3) Decoder Mask')\n",
    "\n",
    "ax1.imshow(enc_mask[:3, 0, 0].numpy(), cmap='Dark2')\n",
    "ax2.imshow(dec_enc_mask[0, 0].numpy(), cmap='Dark2')\n",
    "ax3.imshow(dec_mask[0, 0].numpy(), cmap='Dark2')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787a183d",
   "metadata": {},
   "source": [
    "### 10) 학습률 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0d091fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = step ** -0.5\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        \n",
    "        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "learning_rate = LearningRateScheduler(512)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
    "                                     beta_1=0.9,\n",
    "                                     beta_2=0.98, \n",
    "                                     epsilon=1e-9)\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59af8791",
   "metadata": {},
   "source": [
    "# 2. 데이터 수집 및 가공\n",
    "### 1) 데이터 수집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69451298",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.getenv('HOME')+'/aiffel/transformer/data'\n",
    "kor_path = data_dir+\"/korean-english-park.train.ko\"\n",
    "eng_path = data_dir+\"/korean-english-park.train.en\"\n",
    "\n",
    "# 데이터 정제 및 토큰화\n",
    "def clean_corpus(kor_path, eng_path):\n",
    "    with open(kor_path, \"r\") as f: kor = f.read().splitlines()\n",
    "    with open(eng_path, \"r\") as f: eng = f.read().splitlines()\n",
    "    assert len(kor) == len(eng)\n",
    "\n",
    "    raw = zip(kor, eng)\n",
    "    cleaned_corpus = set(raw)\n",
    "\n",
    "    return cleaned_corpus\n",
    "\n",
    "cleaned_corpus = clean_corpus(kor_path, eng_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2085371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78968 78968\n"
     ]
    }
   ],
   "source": [
    "kor_corpus, eng_corpus = zip(*cleaned_corpus)\n",
    "print(len(kor_corpus), len(eng_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d7a339",
   "metadata": {},
   "source": [
    "### 2) 데이터 가공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3364b3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    \n",
    "    sentence = sentence.lower().strip()\n",
    "\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!가-힣ㄱ-ㅎ]+\", \" \", sentence)\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f5580a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "korean data size: 78968\n",
      "english data size: 78968\n",
      "Korean: 부시는 한반도에 포악한 정권이 두려움과 기아에 살고 있는 국민을 지배하고 있다 고 말했다 .\n",
      "English: on the korean peninsula an oppressive regime rules a people living in fear and starvation bush said .\n"
     ]
    }
   ],
   "source": [
    "enc_corpus = []\n",
    "dec_corpus = []\n",
    "\n",
    "for kor, eng in zip(kor_corpus, eng_corpus):\n",
    "    temp_kor = preprocess_sentence(kor)\n",
    "    temp_eng = preprocess_sentence(eng)\n",
    "\n",
    "    enc_corpus.append(temp_kor)\n",
    "    dec_corpus.append(temp_eng)\n",
    "    \n",
    "print('korean data size:', len(enc_corpus))\n",
    "print('english data size:', len(dec_corpus))\n",
    "print(\"Korean:\", enc_corpus[500])   \n",
    "print(\"English:\", dec_corpus[500])  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c620e227",
   "metadata": {},
   "source": [
    "# 3. 데이터 정제 및 토큰화\n",
    "### 1) sentencepiece 를 통한 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "690be98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "import os\n",
    "\n",
    "# Sentencepiece를 활용하여 학습한 tokenizer를 생성합니다.\n",
    "def generate_tokenizer(corpus, vocab_size, lang=\"ko\", pad_id=0, bos_id=1, eos_id=2, unk_id=3):\n",
    "    model_name = f\"{lang}_spm\"\n",
    "    print('c')\n",
    "\n",
    "    # 텍스트 파일로 코퍼스 저장\n",
    "    temp_file = f\"{model_name}.temp\"\n",
    "    with open(temp_file, 'w', encoding='utf-8') as f:\n",
    "        for line in corpus:\n",
    "            f.write(f'{line}\\n')\n",
    "    print('l')\n",
    "\n",
    "    # SentencePiece 모델 학습\n",
    "    spm.SentencePieceTrainer.Train(\n",
    "        f'--input={temp_file} --model_prefix={model_name} '\n",
    "        f'--vocab_size={vocab_size} --character_coverage=1.0 '\n",
    "        f'--pad_id={pad_id} --bos_id={bos_id} --eos_id={eos_id} --unk_id={unk_id}'\n",
    "    )\n",
    "    print('e')\n",
    "\n",
    "    # 생성된 모델 로드\n",
    "    tokenizer = spm.SentencePieceProcessor()\n",
    "    tokenizer.Load(f\"{model_name}.model\")\n",
    "    print('a')\n",
    "\n",
    "    # 임시 파일 삭제\n",
    "    os.remove(temp_file)\n",
    "    print('r')\n",
    "\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0851729b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#뭐가 잘못되었다\n",
    "# import sentencepiece as spm\n",
    "\n",
    "# def generate_tokenizer(corpus,\n",
    "#                         vocab_size,\n",
    "#                         lang=\"ko\",\n",
    "#                         pad_id=0,\n",
    "#                         bos_id=1,\n",
    "#                         eos_id=2,\n",
    "#                         unk_id=3):\n",
    "#     model_name = f\"{lang}_spm\"\n",
    "\n",
    "#     spm.SentencePieceTrainer.Train(\n",
    "#         '--input={} --model_prefix={} --vocab_size={} --pad_id={} --bos_id={} --eos_id={} --unk_id={}'.format(corpus,model_name, vocab_size, pad_id, bos_id, eos_id, unk_id))\n",
    "#     print('clear')\n",
    "\n",
    "#     # Sentencepiece 사전을 생성합니다.\n",
    "#     tokenizer = spm.SentencePieceProcessor()\n",
    "#     tokenizer.Load('{}.model'.format(model_name))\n",
    "#     pritn('clear2')\n",
    "\n",
    "#     # 임시 파일 삭제\n",
    "#     os.remove(temp_file)\n",
    "\n",
    "#     return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fe757893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "터넨트는 어미 곰의 둔부에 진정제를 놓았지만 근육이 아닌 살에 꽂힌 탓에 약 성분의 흡수가 늦어지고 있었다.\n",
      "The wildlife biologist shoots a tranquilizer dart into the mother's rump, but the dart goes into fat, not muscle, slowing absorption into her blood.\n",
      "터넨트는 어미 곰의 둔부에 진정제를 놓았지만 근육이 아닌 살에 꽂힌 탓에 약 성분의 흡수가 늦어지고 있었다 .\n",
      "the wildlife biologist shoots a tranquilizer dart into the mother s rump but the dart goes into fat not muscle slowing absorption into her blood .\n"
     ]
    }
   ],
   "source": [
    "cleaned_corpus\n",
    "print(kor_corpus[0])\n",
    "print(eng_corpus[0])\n",
    "print(enc_corpus[0])\n",
    "print(dec_corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f7f7da7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c\n",
      "l\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=ko_spm.temp --model_prefix=ko_spm --vocab_size=20000 --character_coverage=1.0 --pad_id=0 --bos_id=1 --eos_id=2 --unk_id=3\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: ko_spm.temp\n",
      "  input_format: \n",
      "  model_prefix: ko_spm\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 20000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 1\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 3\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: 0\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: ko_spm.temp\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 78967 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=5005519\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=1670\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 78967 sentences.\n",
      "unigram_model_trainer.cc(139) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(143) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(194) LOG(INFO) Initialized 160592 seed sentencepieces\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 78967\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 196048\n",
      "unigram_model_trainer.cc(489) LOG(INFO) Using 196048 sentences for EM training\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=83903 obj=12.6761 num_tokens=377937 num_tokens/piece=4.50445\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=71153 obj=11.5149 num_tokens=379275 num_tokens/piece=5.33041\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=53359 obj=11.526 num_tokens=396557 num_tokens/piece=7.43187\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=53340 obj=11.492 num_tokens=396887 num_tokens/piece=7.4407\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=40005 obj=11.6396 num_tokens=420716 num_tokens/piece=10.5166\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=40005 obj=11.603 num_tokens=421237 num_tokens/piece=10.5296\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=30003 obj=11.8072 num_tokens=447679 num_tokens/piece=14.9211\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=30003 obj=11.7639 num_tokens=447678 num_tokens/piece=14.9211\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=22502 obj=12.0095 num_tokens=474538 num_tokens/piece=21.0887\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=22502 obj=11.9619 num_tokens=474529 num_tokens/piece=21.0883\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=22000 obj=11.9888 num_tokens=476538 num_tokens/piece=21.6608\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=22000 obj=11.9844 num_tokens=476577 num_tokens/piece=21.6626\n",
      "trainer_interface.cc(615) LOG(INFO) Saving model: ko_spm.model\n",
      "trainer_interface.cc(626) LOG(INFO) Saving vocabs: ko_spm.vocab\n",
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=en_spm.temp --model_prefix=en_spm --vocab_size=20000 --character_coverage=1.0 --pad_id=0 --bos_id=1 --eos_id=2 --unk_id=3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e\n",
      "a\n",
      "r\n",
      "c\n",
      "l\n",
      "e\n",
      "a\n",
      "r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: en_spm.temp\n",
      "  input_format: \n",
      "  model_prefix: en_spm\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 20000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 1\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 3\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: 0\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: en_spm.temp\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 78956 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=10479543\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=30\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 78956 sentences.\n",
      "unigram_model_trainer.cc(139) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(143) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(194) LOG(INFO) Initialized 83013 seed sentencepieces\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 78956\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 44562\n",
      "unigram_model_trainer.cc(489) LOG(INFO) Using 44562 sentences for EM training\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=34435 obj=10.0197 num_tokens=83334 num_tokens/piece=2.42004\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=25819 obj=8.17663 num_tokens=83826 num_tokens/piece=3.24668\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=21975 obj=8.0916 num_tokens=84670 num_tokens/piece=3.85301\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=21840 obj=8.07198 num_tokens=84917 num_tokens/piece=3.88814\n",
      "trainer_interface.cc(615) LOG(INFO) Saving model: en_spm.model\n",
      "trainer_interface.cc(626) LOG(INFO) Saving vocabs: en_spm.vocab\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# enc_corpus = []\n",
    "# dec_corpus = []\n",
    "\n",
    "SRC_VOCAB_SIZE = TGT_VOCAB_SIZE = 20000\n",
    "\n",
    "ko_tokenizer = generate_tokenizer(enc_corpus, SRC_VOCAB_SIZE, \"ko\")\n",
    "en_tokenizer = generate_tokenizer(dec_corpus, TGT_VOCAB_SIZE, \"en\")\n",
    "en_tokenizer.set_encode_extra_options(\"bos:eos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "065dc153",
   "metadata": {},
   "outputs": [],
   "source": [
    "#원활한 진행을 위하여 변수 카피 진행\n",
    "import copy\n",
    "\n",
    "kor_corpus = copy.deepcopy(enc_corpus)\n",
    "eng_corpus = copy.deepcopy(dec_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b1ae9c22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86545cad41d74343b73f0fc9cd877919",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78968 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tqdm.notebook import tqdm    # 진행 과정을 보기 위한 도구\n",
    "\n",
    "src_corpus = []\n",
    "tgt_corpus = []\n",
    "\n",
    "assert len(kor_corpus) == len(eng_corpus)\n",
    "\n",
    "# 토큰의 길이가 50 이하인 문장만 남깁니다.\n",
    "for idx in tqdm(range(len(kor_corpus))):\n",
    "    src_tok = ko_tokenizer.EncodeAsIds(kor_corpus[idx])  # 한국어 문장 토큰화\n",
    "    tgt_tok = en_tokenizer.EncodeAsIds(eng_corpus[idx])  # 영어 문장 토큰화\n",
    "\n",
    "    if len(src_tok) <= 50 and len(tgt_tok) <= 50:  # 토큰 길이가 50 이하인 경우만 선택\n",
    "        src_corpus.append(src_tok)\n",
    "        tgt_corpus.append(tgt_tok)\n",
    "\n",
    "# 패딩처리를 완료하여 학습용 데이터를 완성합니다.\n",
    "enc_train = tf.keras.preprocessing.sequence.pad_sequences(src_corpus, padding='post')\n",
    "dec_train = tf.keras.preprocessing.sequence.pad_sequences(tgt_corpus, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f54908a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁부시', '▁대통령이', '▁탈북자들', '을', '▁만난', '▁것은', '▁년', '▁월', '▁강철', '환', '씨', '를', '▁집무실', '로', '▁초대', '한', '▁이후', '▁이번', '이', '▁두', '▁번째', '이다', '▁.']\n",
      "['▁터넨트', '는', '▁어미', '▁곰', '의', '▁둔', '부', '에', '▁진정제', '를', '▁놓았', '지만', '▁근육', '이', '▁아닌', '▁살', '에', '▁', '꽂', '힌', '▁', '탓', '에', '▁약', '▁성분', '의', '▁흡수', '가', '▁', '늦', '어', '지', '고', '▁있었다', '▁.']\n",
      "['▁약물', '▁치료', '를', '▁하는', '▁고혈압', '▁환자들', '은', '▁손', '으로', '▁하는', '▁마사지', '를', '▁피해', '야', '▁한다', '▁.']\n",
      "['▁한편', '▁이라크', '▁북부', '▁지역', '의', '▁최고', '▁미', '▁사령관', '은', '▁이', '▁지역에', '▁연합군', '이', '▁년', '▁반', '▁동안', '▁더', '▁주둔', '해야', '▁한다고', '▁지적했다', '▁.']\n",
      "['▁케네디', '는', '▁타임즈', '에서', '▁자신이', '▁상원의원직', '에', '▁가장', '▁적합한', '지에', '▁대한', '▁질문에', '▁가장', '▁적합', '하다고', '▁생각하지', '▁않았다면', '▁여기', '▁있지', '▁않을', '▁것', '▁이라고', '▁말했다', '▁.']\n",
      "['▁그러나', '▁많은', '▁여성', '▁단체들', '과', '▁예술가들', '은', '▁법에', '▁의해', '▁외', '설', '로', '▁치', '부', '되는', '▁것', '들이', '▁법적', '인', '▁제재', '를', '▁받게', '되는', '▁경우', '▁표현', '의', '▁자유', '까지', '도', '▁침해', '할', '▁수', '▁있다고', '▁우려', '하고', '▁있다', '▁.']\n",
      "['▁침실', '은', '▁개인', '의', '▁성', '역', '이지만', '▁자신의', '▁취미', '나', '▁취향', '을', '▁보여주', '게', '▁된다', '▁.']\n",
      "['▁멕시코', '▁재벌', '▁칼로', '스', '▁슬림', '이', '▁자산', '규모', '▁억달러', '▁약', '▁조원', '▁로', '▁마이크로소프트', '사의', '▁빌', '▁게이츠', '를', '▁뛰어넘', '어', '▁세계', '▁위', '▁부자', '로', '▁등', '극', '했다고', '▁멕시코', '▁재무', '재', '산', '▁관계자가', '▁일', '▁현지시간', '▁발표했다', '▁.']\n",
      "['▁한편', '▁올', '잉', '글', '랜드', '클', '럽', '은', '▁이번', '▁대회', '부터', '▁호', '크', '아', '이', '▁기술', '을', '▁처음', '으로', '▁사용한다', '고', '▁밝혔다', '▁.']\n",
      "['▁오', '자와', '의', '▁발표', '는', '▁그', '가', '▁일본의', '▁새', '▁총리', '의', '▁연정', '▁제안', '을', '▁즉', '시', '▁거절', '하지', '▁않은', '▁것에', '▁대한', '▁당내', '▁비판', '을', '▁수용', '한', '▁이후', '▁나왔다', '▁.']\n"
     ]
    }
   ],
   "source": [
    "# 원하는 인덱스의 데이터에 대해서만 수행\n",
    "# 정현님 코드 참조\n",
    "\n",
    "for i in range(10):\n",
    "    src_sequence = enc_train[i - 1]    \n",
    "    src_tokens = [ko_tokenizer.IdToPiece(int(token)) for token in src_sequence if token != 0]\n",
    "#     print(src_sequence)\n",
    "    print(src_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a65adf7",
   "metadata": {},
   "source": [
    "# 4. 훈련하기 \n",
    "### 1) 트렌스포머 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6751579d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LearningRateScheduler\n",
    "\n",
    "# 트랜스포머 선언\n",
    "\n",
    "# Hyperparameters\n",
    "N_LAYERS = 2\n",
    "D_MODEL = 512\n",
    "N_HEADS = 8\n",
    "D_FF = 2048\n",
    "POS_LEN = 200  # 이 값은 데이터셋에 따라 조정될 수 있습니다.\n",
    "DROPOUT = 0.1\n",
    "\n",
    "# Transformer 인스턴스 생성\n",
    "transformer = Transformer(\n",
    "    n_layers=N_LAYERS,\n",
    "    d_model=D_MODEL,\n",
    "    n_heads=N_HEADS,\n",
    "    d_ff=D_FF,\n",
    "    src_vocab_size=SRC_VOCAB_SIZE,\n",
    "    tgt_vocab_size=TGT_VOCAB_SIZE,\n",
    "    pos_len=POS_LEN,\n",
    "    dropout=DROPOUT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2bcaf8",
   "metadata": {},
   "source": [
    "### 2) 손실함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ff2257e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    # Masking 되지 않은 입력의 개수로 Scaling하는 과정\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9815e89d",
   "metadata": {},
   "source": [
    "### 3) Train step 함수 선정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b8395a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_step 함수 완성\n",
    "@tf.function()\n",
    "def train_step(src, tgt, model, optimizer):\n",
    "    gold = tgt[:, 1:]\n",
    "\n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = model(src, tgt, enc_mask, dec_enc_mask, dec_mask)\n",
    "        loss = loss_function(gold, predictions[:, :-1])\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    return loss, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c55fd69",
   "metadata": {},
   "source": [
    "### 4) attention 시각화 함수 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "13b70799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention 시각화 함수\n",
    "\n",
    "def visualize_attention(src, tgt, enc_attns, dec_attns, dec_enc_attns):\n",
    "    def draw(data, ax, x=\"auto\", y=\"auto\"):\n",
    "        import seaborn\n",
    "        seaborn.heatmap(data, \n",
    "                        square=True,\n",
    "                        vmin=0.0, vmax=1.0, \n",
    "                        cbar=False, ax=ax,\n",
    "                        xticklabels=x,\n",
    "                        yticklabels=y)\n",
    "        \n",
    "    for layer in range(0, 2, 1):\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        print(\"Encoder Layer\", layer + 1)\n",
    "        for h in range(4):\n",
    "            draw(enc_attns[layer][0, h, :len(src), :len(src)], axs[h], src, src)\n",
    "        plt.show()\n",
    "        \n",
    "    for layer in range(0, 2, 1):\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        print(\"Decoder Self Layer\", layer+1)\n",
    "        for h in range(4):\n",
    "            draw(dec_attns[layer][0, h, :len(tgt), :len(tgt)], axs[h], tgt, tgt)\n",
    "        plt.show()\n",
    "\n",
    "        print(\"Decoder Src Layer\", layer+1)\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        for h in range(4):\n",
    "            draw(dec_enc_attns[layer][0, h, :len(tgt), :len(src)], axs[h], src, tgt)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60734047",
   "metadata": {},
   "source": [
    "### 5) 번역 생성 함수 선정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "68efb2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 번역 생성 함수\n",
    "\n",
    "def evaluate(sentence, model, src_tokenizer, tgt_tokenizer):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    pieces = src_tokenizer.encode_as_pieces(sentence)\n",
    "    tokens = src_tokenizer.encode_as_ids(sentence)\n",
    "\n",
    "    _input = tf.keras.preprocessing.sequence.pad_sequences([tokens],\n",
    "                                                           maxlen=enc_train.shape[-1],\n",
    "                                                           padding='post')\n",
    "    \n",
    "    ids = []\n",
    "    output = tf.expand_dims([tgt_tokenizer.bos_id()], 0)\n",
    "    for i in range(dec_train.shape[-1]):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = \\\n",
    "        generate_masks(_input, output)\n",
    "\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns =\\\n",
    "        model(_input, \n",
    "              output,\n",
    "              enc_padding_mask,\n",
    "              combined_mask,\n",
    "              dec_padding_mask)\n",
    "\n",
    "        predicted_id = \\\n",
    "        tf.argmax(tf.math.softmax(predictions, axis=-1)[0, -1]).numpy().item()\n",
    "\n",
    "        if tgt_tokenizer.eos_id() == predicted_id:\n",
    "            result = tgt_tokenizer.decode_ids(ids)\n",
    "            return pieces, result, enc_attns, dec_attns, dec_enc_attns\n",
    "\n",
    "        ids.append(predicted_id)\n",
    "        output = tf.concat([output, tf.expand_dims([predicted_id], 0)], axis=-1)\n",
    "\n",
    "    result = tgt_tokenizer.decode_ids(ids)\n",
    "\n",
    "    return pieces, result, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b2427ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 번역 생성 및 Attention 시각화 결합\n",
    "\n",
    "def translate(sentence, model, src_tokenizer, tgt_tokenizer, plot_attention=False):\n",
    "    pieces, result, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "    evaluate(sentence, model, src_tokenizer, tgt_tokenizer)\n",
    "    \n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "\n",
    "    if plot_attention:\n",
    "        visualize_attention(pieces, result.split(), enc_attns, dec_attns, dec_enc_attns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e83d4420",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_177/1811356450.py:20: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  t = tqdm_notebook(idx_list)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5a8b693f5794224873cca74872fba28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama s obama is a victory .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the city of the city s .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: the united states is a good .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: the city of the city of the city .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98c149ee6bd5434c928bd01a14ce684a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama is a president elect barack obama .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the city is a city of the city .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: coffee coffee there is no .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: the death toll was killed .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e85d508afd754699aacfb18f5ced6f28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: the president s president obama s aides .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the city s urban city is the streets .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: no coffees .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: seven died tuesday when the crashed .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "252a5abae3e44ec4a563a5fb1c598e4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama is doing .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the city is the city of urban city .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: don t need to do anything .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: the death toll from the people were killed .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4921ff8234bc4d93b78eceede14af2b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama is the president .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the citizens are a target .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: coffee need doesn t need for coffee .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: seven people died monday when the plane crashed into the city sunday .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c21813c6d3d4506bf69414009c13137",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama is a point .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: people in urban city .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: no need for no need for no .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: seven people were killed monday night without the deaths .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ea4ee27317b4acababfed7a689f7cca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama is the president for nearly years .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: citizens are similar to the mayor .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: no he needs to defend coffee .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: seven people were killed .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaa5a7c5b05647a8bd0f099af38fa692",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama is the president .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: they re in the city s urban city .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: he needs to take a special or .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: seven people seven died when seven came seven days .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19236e6bcda84f3684c4dab5c589f39b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama is the illinois senator .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: street .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: need for coffee .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: seven other people were killed .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e343e80680cd40c18173b0a13f4c0fb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama is a strong president .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: people are through the city of long .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: no need to hold necessary .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: seven people seven died seventh on saturday night .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d9063a6bfd74891ae9fb92605759ce5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama is the only republican .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: citizens are similar to the city s good .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: no needs to coffee .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: seven seven seven seven seven seven seven seven seven seven seven seven years ago seven will die .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "914e1de033d14534be43cf884ceb39f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: people will be homeless .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: no he needs to do he .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: six people were killed seven people .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c881a6603f7482b835d0208f5b8aa4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama s presidential campaign .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: people will mayor s city only .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: no he needs to take coffee .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: seven people were killed seven people .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dcfa289c2974e2cb401c29d36031686",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama is the same term .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: and they re mayor of s ahead .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: no needs to be coffee .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: seven people were killed seven people .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54ac80d7ee904de8a30ddcca8acb87b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama is only a list of basis .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: people will mayor us a baby .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: no he needs to take coffee .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: seven died seven people when seventh were killed .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef840c05479c4bbfad6f6682a311518a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama is the second president .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: citizens are vowing to take place in the city .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: no needs to take coffee .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: seven died seven days earlier . regarded the injuries .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea2856cbb84d48b7b753c204e1140359",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: the president obama is said to be nearly a surveillance .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: to city urban rural street .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: no needs to take coffee .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: seven people were killed and seven were killed thursday .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d73c8d64beb7434bab7e8e727ec9c5c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama campaigns .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: mayor .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: he needs to take coffee .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: seven other people were killed thursday when seven people were killed .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f603a240ba984f50bbaec44bef803e52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama is the only son .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: people will see their city s stop .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: no need to take coffee .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: seven other people were killed .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "374c07c89e7546ab9a4d5ef85cfa9fb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama is the president s passion .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: people will mayor to take their city .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: coffee needs .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: seven other people were killed .\n"
     ]
    }
   ],
   "source": [
    "# 학습\n",
    "\n",
    "from tqdm import tqdm_notebook \n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20\n",
    "\n",
    "examples = [\n",
    "            \"오바마는 대통령이다.\",\n",
    "            \"시민들은 도시 속에 산다.\",\n",
    "            \"커피는 필요 없다.\",\n",
    "            \"일곱 명의 사망자가 발생했다.\"\n",
    "]\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    \n",
    "    idx_list = list(range(0, enc_train.shape[0], BATCH_SIZE))\n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm_notebook(idx_list)\n",
    "\n",
    "    for (batch, idx) in enumerate(t):\n",
    "        batch_loss, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        train_step(enc_train[idx:idx+BATCH_SIZE],\n",
    "                    dec_train[idx:idx+BATCH_SIZE],\n",
    "                    transformer,\n",
    "                    optimizer)\n",
    "\n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        t.set_description_str('Epoch %2d' % (epoch + 1))\n",
    "        t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))\n",
    "\n",
    "    for example in examples:\n",
    "        translate(example, transformer, ko_tokenizer, en_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593d0698",
   "metadata": {},
   "source": [
    "# 회고 \n",
    "\n",
    "- 모델 구축하여 번역문까지 결과를 확인하였습니다.\n",
    "- 토큰화하는 과정에서 오류가 있었지만 해결하였다.\n",
    "- 모델 구조에 대한 학습이 조금 더 필요하다고 생각이 든다.\n",
    "- 번역 및 토큰화하는 과정에 숫자를 제거하고 진행하였는데 숫자를 추가하여 번역을 진행하면 성능개선이 이루어지지 않을까 한다.\n",
    "- 다른 데이터 셋으로 학습을 진행하면 더 나은 성능이 나올까 하는 궁금증이 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ab6aa3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
